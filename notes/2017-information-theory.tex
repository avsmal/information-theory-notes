% !TeX spellcheck = ru_RU

\documentclass[12pt]{article}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{xspace}
\usepackage{fullpage}

%% CALCULATOR
\usepackage{calc}
%% STYLE
\usepackage[raggedright,small]{titlesec}
\usepackage[dotinlabels]{titletoc}
\titlelabel{\thetitle.\quad}
%% Puts "." instead of ":" in captions
\usepackage{ccaption}
\captiondelim{. }

%\usepackage{indentfirst}
%% OTHER

\usepackage[bookmarks=false, colorlinks, unicode, pdfstartview=FitH, pdftex]{hyperref}
\hypersetup{ 
 plainpages=true,
 linkcolor=blue,
 citecolor=red,
 menucolor=blue,
 pdfnewwindow=true
}

\usepackage{tikz}
\usetikzlibrary{positioning}

\newcommand{\bits}{\{0,1\}}
\newcommand{\bitstr}{\bits^*}
\newcommand{\sshalf}{{\textstyle\frac12}}
\newcommand{\seqn}[2]{{#1}_1,{#1}_2,\dotsc,{#1}_{#2}}
\newcommand{\seqin}[3]{{#1}_{{#2}_1},{#1}_{{#2}_2},\dotsc,{#1}_{{#2}_{#3}}}
\newcommand{\IC}{\mathrm{IC}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\Nat}{\mathbb{N}}

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\rng}{rng}

\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Теорема}[section]
\newtheorem{lemma}{Лемма}[section]
\newtheorem{statement}{Утверждение}[section]
\newtheorem{corollary}{Следствие}[section]

\theoremstyle{remark}
\newtheorem{example}{Пример}[section]
\newtheorem{exercise}{Упражнение}[section]
\newtheorem{remark}{Замечание}[section]
\newtheorem{problem}{Задача}[section]

\newenvironment{tasks}{\paragraph{Задачи.}\begin{enumerate}}{\end{enumerate}}

%opening
\title{Заметки к курсу ,,Теория информации`` в СПбАУ}
\author{А.В. Смаль}

\begin{document}

\maketitle

\begin{abstract}
Курс посвящён изучению подходов к определению понятия  ,,количество информации``. 
Последовательность изложения материала данного курса основана на классической статье Колмогорова ,,Три подхода к
определению понятия количества информации`` (1965).

В курсе будет рассмотрено три подхода к определению ,,количества информации``: 
комбинаторный (информация по Хартли), вероятностный (энтропия Шеннона) и 
алгоритмический (Колмогоровская сложность). Кроме этого мы поговорим про
различные применения аппарата теории информации в различных областях компьютерных наук:
в криптографии, в коммуникационной сложности, в теории кодирования, 
в теории конечных автоматов, в теории сложности вычислений и некоторых других.
\end{abstract}

\newpage\tableofcontents\newpage

\section{Комбинаторный подход}
\subsection{Информация по Хартли}
Пусть задано некоторое конечное множество \(A\)~--- \emph{множество исходов}.
\begin{definition}[1928]
Определим \emph{количество информации в \(A\)} как \(\chi(A) = \log_2|A|\) (мы будем измерять количество информации в битах, поэтому все логарифмы будут по основанию \(2\), для байтов основание нужно было бы заменить на \(256\)).
\end{definition}

Если про некоторый \(x\in A\) стало известно, что \(x\in B\), то теперь для идентификации \(x\) нам достаточно \(\chi(A\cap B) = \log |A\cap B|\) битов, т.е. нам сообщили \(\chi(A) - \chi(A\cap B)\) битов информации.

\begin{example}
    Предположим, что мы хотим узнать некоторое неизвестное упорядочение множества $\{\seqn{a}{5}\}$. Нам стало известно,
    что \(a_1>a_2\) или \(a_3>a_4\). Сколько битов информации мы узнали? Множество \(A\) состоит из \(5!\) перестановок,
    множество \(B\)~--- из перестановок, которые удовлетворяют новому условию. Легко проверить, что \(|B| = 90\). Итого
    мы узнали \(\log 120 - \log 90 = \log(4/3)\) битов.
\end{example}

Пусть \(A\subset\bitstr\times\bitstr\). Обозначим через \(\pi_1(A)\) и \(\pi_2(A)\) проекции множества \(A\) на первую и вторую координату соответственно, а \(\chi_1(A) = \log|\pi_1(A)|\) и \(\chi_2(A) = \log|\pi_2(A)|\)~--- их сложность по Хартли.

\begin{theorem} 
\(\chi(A) \le \chi_1(A) + \chi_2(A)\).
\end{theorem}

\begin{definition}
Количество информации в второй компоненте \(A\) при известной первой
\[\chi_{2|1} = \log\left(\max_{a\in\pi_1(A)} |A_a|\right),\]
где $A_a = \{(a, x) \mid x\in \pi_2(A)\}$.
\end{definition}

\begin{theorem} 
\(\chi(A) \le \chi_1(A) + \chi_{2|1}(A)\).
\end{theorem}

\begin{theorem}\label{thm:volume}
Для \(A\subset\bitstr\times\bitstr\times\bitstr\)
\[2\cdot\chi(A) \le \chi_{12}(A) + \chi_{13}(A) + \chi_{23}(A).\]
\end{theorem}
\begin{corollary}
Квадрат объёма трёхмерного тела не превосходит произведение площадей его проекций на координатные плоскости.
\end{corollary}

\begin{statement}
Если \(f: X\to Y\)
\begin{enumerate}
    \item является сюръекцией, то \(\chi(Y)\le \chi(X)\),
    \item является инъекцией, то \(\chi(X)\le \chi(Y)\).
\end{enumerate}
\end{statement}

\subsection{Применение: игра в 10 вопросов}
Сколько вопросов на ДА/НЕТ нужно задать, чтобы определить загаданное число от 1 до \(N\), если (a) можно задавать вопросы адаптивно; (б) вопросы нужно написать на бумажке заранее.

Оценка \(\lceil\log N\rceil \) достигается в обоих случаях, если задавать вопросы про биты двоичного представления загаданного числа.

Докажем нижнюю оценку. Пусть \(A=[N]\). Множество \(Q = \{(\seqn{q}{k})\}\)~--- множество протоколов (ответы на вопросы). 
Можно рассматривать \(A\) и \(Q\) как проекции некоторого множества исходов игры \(S\) на разные координаты. Тогда верны следующие неравенства:
\begin{itemize}
\item \( \chi_Q(S) = \chi(Q) \le \chi_1(Q) + \chi_2(Q) + \dotsb + \chi_k(Q) \le k, \)
\item \( \chi_A(S) = \chi(A) \le \chi(S) \le \chi_Q(S) + \chi_{A|Q}(S) \le k + 0 = k. \)
\end{itemize}
Таким образом получаем, что \(\log N = \chi(A) \le k\).

\subsection{Цена информации}
Пусть имеется некоторое неизвестное число от 1 до \(n\) (где \(n\ge2)\).
Разрешается задавать любые вопросы с ответами ДА/НЕТ. При ответе ДА мы
заплатим 1 рубль, а при ответе НЕТ~— два рубля. Сколько необходимо и достаточно заплатить для отгадывания числа?

\paragraph{Верхняя оценка.} Давайте задавать вопросы так, чтобы отрицательные ответы приносили в два раза больше информации, чем положительные. Тогда за каждый бит информации мы заплатим \(c\log n\) для некоторой константы \(c\). Пусть вопросы будут вида ,,\(x\in T\)?``. Тогда требуется
\[2(\log |X| - \log|X \cap T|) = \log |X| - \log|X\cap\overline T|.\]
Пусть \(|X \cap T| = \alpha|X|\), тогда \(|X\cap\overline T| = (1 - \alpha)|X|\),
т.о. \(\alpha^2 = 1 - \alpha\), \(\alpha=(\sqrt 5 - 1) / 2\). При любом ответе мы заплатим \(c = 1/(-\log \alpha)\approx 1.44\) рублей за бит, а в целом~— \(\log n / (-\log\alpha)\) рублей.

\paragraph{Нижняя оценка.} Применим рассуждение про злонамеренного противника (adversary argument). Пусть противник
выбирает ответ ДА/НЕТ в зависимости от того, какое из двух значение \(1/(\log |X| - \log|X \cap T|)\) и \((2/\log |X| -
\log|X \cap \overline T|)\) больше. При любых \(X,\ T\) одно из этих значений не меньше \(c = 1/(-\log\alpha)\). Таким
образом мы заставляем алгоритм платить не менее \(c\) рублей за бит, а значит любой алгоритм в худшем случае заплатит
\(\lceil c\log n\rceil\) рублей.

\subsection{Применение: упорядочивание камней по весу}
\subsubsection{Верхняя и нижняя оценки для произвольного $N$}
Сколько сравнений нужно сделать для того, чтобы упорядочить \(N\) камней по весу?

\paragraph{Нижняя оценка.} Потребуется \(\lceil\chi(S_N)\rceil = \lceil\log n!\rceil\) сравнений.  

\paragraph{Верхняя оценка.} Будем сортировать вставкой с бинарным поиском места вставки. Количество сравнений:
\[
\lceil\log 2\rceil + \lceil\log 3\rceil +\dotsb+ \lceil\log n\rceil \le \log n! + n - 1 = n\log n + O(n).
\]

\subsubsection{Точные оценки для маленьких $N$}
\begin{exercise}
Сколько нужно взвешиваний, чтобы упорядочить \(N\) камней по весу? 
Найдите точный ответ на этот вопрос для \(N = 2, 3, 4, 5\). Указание: воспользуйтесь жадной стратегией, при которой каждое взвешивание приносит максимум информации.
\end{exercise}


\subsection{Применение: поиск фальшивой монетки}
\begin{itemize}
\item 20 монет, одна фальшивая легче остальных.

Каждое взвешивание даёт не более \(\log 3\) битов. 
Итого \(k\ge\log N/\log 3 = \log_3 N\).

\item 13 монет, одна фальшивая (с неизвестным относительным весом), 3 взвешивания.

Два варианта первого шага:
\begin{itemize}
\item если взвешиваем по 4, то при равенстве нельзя из 5 за два взвешивания найти фальшивую (остаётся 10 исходов),
\item если взвешиваем по 5, то при неравенстве остаётся 10 возможных исходов.
\end{itemize}

\item 15 монет, одна фальшивая, три взвешивания. Не требуется узнавать относительный вес монеты.

Всего исходов \(2\cdot 14 + 1 > 27\), т.к. только в случае трёх равенств мы можем не узнать относительный вес фальшивой монеты.

\item 14 монет, одна фальшивая, три взвешивания. Не требуется узнавать относительный вес монеты.

Всего исходов \(2\cdot 13 + 1 \le 27\), но определить тем не менее нельзя. Аппарата информации по Хартли недостаточно.

\end{itemize}

\begin{exercise}
За 4 взвешивания одну найти фальшивую монету из 12, если её относительный вес неизвестен. Указание: воспользуйтесь ,,жадной`` стратегией, при которой каждое взвешивание приносит максимум информации.
\end{exercise}

\begin{exercise}
Пусть \(L_n\)~— множество путей длины \(n\) в графе.
\begin{center}
\begin{tikzpicture}
\tikzstyle{vert}=[circle, draw, fill=black!50,
                        inner sep=0pt, minimum width=4pt]
\node[vert, label=below:0] (a) at (1,0) {}; 
\node[vert, label=below:1] (b) at (3,0) {};
\node (c) at (0,0) {};
\path
      (a) edge [->,bend left] (b)
      (b) edge [->,bend left] (a)
      (a) edge [bend left, in=90] (0,0)
      (0,0) edge [->,bend left,out=90] (a);
\end{tikzpicture}
\end{center}
Чему равен предел \(\lim_{n\to\infty} \frac{\chi(L_n)}{n}\)?
\end{exercise}

\begin{exercise}
Пусть загадано число от 1 до \(N\). Можно задавать любые вопросы на ДА/НЕТ. Сколько вопросов потребуется, если на один ответ можно дать неверный ответ, а вопросы (а) можно задавать адаптивно; (б) нужно написать заранее?
\end{exercise}

\subsection{Логика знаний}
В этом разделе мы будем называть множество исходов $A$ множеством \emph{миров}.
Пусть $f$~--- это некоторая функция из $A$ в некоторое множество $I$ (будем воспринимать это как информация о мире).
Нам не важно какие значения принимает $f$, нам будут важны лишь классы эквивалентности, на которые $f$ разбивает $A$:
каждый класс эквивалентности будет состоять из миров $A$ с одинаковым значением $f$.

\begin{example}
    Пусть $A = \{1,2,3,4,5\}$, а $f(x) = x \bmod 3$. Тогда $f$ разбивает $A$ на три класса эквивалентности 
    $\{1,4\}$, $\{2,5\}$ и $\{3\}$.
\end{example}

Пусть $B\subset A$~--- это некоторое \emph{утверждение} о мирах. $B$ \emph{истинно} в мире $x$, если $x\in B$.
В противном случае $B$ \emph{ложно} в $x$. В мире $x$ мы \emph{знаем, что $B$ истинно}, если $y \in B$ для всех 
$y\sim x$.

\begin{example}
    Пусть $A = \{1,2,3,4,5\}$, а $f(x) = x \bmod 3$. Тогда в мирах $1$, $4$ и $3$ мы знаем, 
    что мир меньше $5$.  А в мирах $2$ и $5$~--- не знаем.
\end{example}
\begin{remark}
    ,,Не знаем`` мы будем понимать в смысле ,,не верно, что знаем``.
\end{remark}

К утверждениям о мирах можно применять обычные логические связки: <<И>> (пересечение), <<ИЛИ>> (объединение),
<<НЕ>> (дополнение).

\begin{statement}
    Если в мире $x$ мы знаем $B$, то в мире $x$ мы знаем, что мы знаем $B$.
    Аналогично, если в мире $x$ мы не знаем $B$, то в мире $x$ мы знаем, что не знаем $B$.
\end{statement}

Пусть теперь у нас есть $k$ человек со своими знаниями о мире. 
Они определяют $k$ отношений эквивалентности $\sim_1,\sim_2,\dotsc,\sim_k$ и,
соответственно, $k$ разбиений на классы эквивалентности.

\begin{example}
    Пусть множество миров $A = \{1,2,3,4,5\}$ и есть два человека, Алиса и Боб.
    Алиса знает значения $f_A(x) = x \bmod 3$, а Боб знает $f_B(x) = x\bmod 2$.
    Тогда классы эквивалентности Алисы: $\{1,4\}$, $\{2,5\}$ и $\{3\}$,
    а классы эквивалентности Боба:  $\{1,3,5\}$ и $\{2,4\}$.
    В мире 1 Алиса знает, что мир меньше 5, а Боб не знает. В мире 4 они оба это знают.
    В мире 1 Алиса не знает, что Боря не знает, что мир меньше 5 (действительно, в мире 4,
    который с точки зрения Алисы эквивалентен 1, Боря это знает).
\end{example}

\begin{problem}
    Пусть имеется некоторая карточка, про которую известно, 
    что на одной её стороне написано целое неотрицательное
    число $n$, а на другой~--- целое число $n+1$.
    Алиса и Боб сидят друг напротив друга смотрят на эту карточку с разных сторон и между ними происходит
    следующий разговор.
    \begin{itemize}
        \item[А:] Я не знаю числа на стороне Боба.
        \item[Б:] Я не знаю числа на стороне Алисы.
    \end{itemize}
    Это повторяется 10 раз и после этого Алиса говорит, что знает число на стороне Боба. Какие числа могли быть написаны
    на карточке?
\end{problem}

\begin{problem}
    В магазине имеется три красные шляпы и две белые. Три джентльмена по-очереди покупают случайную шляпу и не глядя 
    надевают её на себя (т.е. джентльмен не знает цвета шляпы, которую он купил). После этого джентльмены смотрят друг
    на друга и происходит следующий разговор.
    \begin{itemize}
        \item[1:] Я не знаю цвета своей шляпы.
        \item[2:] Я не знаю цвета своей шляпы.
        \item[3:] Теперь я знаю цвет своей шляпы.
    \end{itemize}
    Какого цвета шляпа на третьем джентльмене?
\end{problem}


\section{Вероятностный подход}
\subsection{Энтропия Ш\'{е}ннона}

Энтропия Шеннона определяет количество информации \(H(\alpha)\) в распределении
вероятностей для некоторой случайной величины \(\alpha\).  Пусть \(\alpha\)
принимает значения из множества \(\{\seqn{a}{k}\}\) с вероятностями
\(\{\seqn{p}{k}\}\), \(p_i\ge 0 \), \(\sum_ip_i=1\).

Нам бы хотелось, чтобы это определение согласовывалось с определением Хартли, т.е. имеют место следующие ,,граничные условия``:
\begin{itemize}
\item если \(p_1=\dotsb=p_k\), то \(H(\alpha) = \log k\),
\item если \(p_1=1\), \(p_2=\dotsb=p_k=0\), то \(H(\alpha) = 0\).
\end{itemize}
Будем искать \(H(\alpha)\) в виде математического ожидания ,,удивления`` от исхода случайной величины (,,удивление`` зависит от вероятности данного исхода).
\[H(\alpha) = \sum_i p_i\cdot \mathrm{impress}(p_i).\]
Граничные условия однозначно определяют функцию \(\mathrm{impress}(p_i) = \log\frac{1}{p_i} = -\log p_i\).
\begin{definition}[1948]
Энтропия Шеннона случайной величины \(\alpha\) 
\[
H(\alpha) = \sum_{i=1}^k p_i\cdot\log\frac1p_i.
\]
(По непрерывности доопределим \(0\cdot \log\frac10 = 0\).)
\end{definition}

Можно вывести это соотношение из определения Хартли более формально. 
Пусть $W_n$~--- это множество всех слов длины $n$ состоящих из букв $\{\seqn{a}{k}\}$,
где каждая буква $a_i$ встречается ровно $n_i = p_i\cdot n$ раз
(будем считать, что вероятности $p_i$ рациональны, и что множество 
$W_n$ определено только тогда, когда все $n_i$ целые).
Информация по Хартли в $W_n$ 
\[
    \chi(W_n) = \log |W_n| = \log \frac{n!}{n_1!n_2!\dotsb n_k!}.
\]
Это выражение можно оценить при помощи формулы Стирлинга.
\[
    \begin{aligned}
    \chi(W_n)&\ =  \log \frac{\poly(n)\cdot (n/e)^n}
    {\poly(n)\cdot(n_1/e)^{n_1}\cdot(n_1/e)^{n_2}\dotsm(n_k/e)^{n_k}} =\\
    &\ = \log \left(\left(\frac{n}{n_1}\right)^{n_1}\cdot
                    \left(\frac{n}{n_2}\right)^{n_2}\dotsm
                    \left(\frac{n}{n_k}\right)^{n_k}\right) + O(\log n) =\\
    &\ = \log \left(\left(\frac{1}{p_1}\right)^{p_1\cdot n}\cdot
                    \left(\frac{1}{p_2}\right)^{p_2\cdot n}\dotsm
                    \left(\frac{1}{p_k}\right)^{p_k\cdot n}\right) + O(\log n) =\\
    &\ = n \cdot \sum_{i=1}^k p_i \cdot \log\frac{1}{p_i} + O(\log n).
    \end{aligned}
\]
В среднем на один символ приходится $\chi(W_n)/n$ битов информации.
В пределе получаем
\[
    \lim_{n\to \infty}\frac{\chi(W_n)}{n} = \sum_{i=1}^k p_i \cdot \log\frac{1}{p_i} = H(\alpha)
\]
(предел нужно брать по бесконечной подпоследовательности натуральных чисел $n$ таких,
для которых все $\{n_i\}$~--- целые).
 

\begin{lemma}\label{lm:entropy-properties}
Для энтропии Шеннона выполняются следующие соотношения.
\begin{itemize}
\item \(H(\alpha) \ge 0\), причём \(H(\alpha) = 0\) \(\iff\) распределение \(\alpha\) вырождено.

\item \(H(\alpha) \le \log k\), причём \(H(\alpha) = \log k\) \(\iff\) величина \(\alpha\) распределена равномерно.

\end{itemize}
\end{lemma}

Для доказательства нам потребуется следующая теорема.

\begin{theorem}[Неравенство Йенсена]
Пусть функция \(f ( x ) \) является вогнутой на некотором промежутке \(\mathcal {X}\)
и числа \(\seqn{q}{n}>0\) таковы, что \(q_1+\ldots +q_{n}=1\). 
Тогда для любых \(\seqn{x}{n}\) из промежутка \(\mathcal X\) выполняется неравенство:
\[
\sum _{{i=1}}^{{n}}q_{i}f(x_{i})\leq f\left(\sum _{{i=1}}^{{n}}q_{i}x_{i}\right). 
\]
\end{theorem}
\begin{proof}[Доказательство леммы \ref{lm:entropy-properties}]
Первое свойство следует напрямую из определения: каждый член суммы \(H(\alpha)\) неотрицателен и равен нулю только в случае, если \(p_i = 0\) или \(p_i = 1\).

Для доказательства второго неравенства перенесём всё в левую часть и применим неравенство Йенсена:
\[
H(\alpha) - \log k 
= \sum_{i=1}^k p_k\cdot\log\frac{1}{p_i} - \sum_{i=1}^k p_i\cdot\log k 
= \sum_{i=1}^k p_k\cdot\log\frac{1}{p_ik} 
\le \log\left(\sum_{i=1}^k p_i\frac{1}{p_i k}\right) = \log 1 = 0.
\]
\end{proof}
Энтропию совместного распределения пары случайных величин \(\alpha\) и \(\beta\) будем обозначать \(H(\alpha,\beta)\).
\begin{lemma}
Выполняются следующие свойства:
\begin{itemize}
    \item \(H(\alpha, \beta) \le H(\alpha) + H(\beta)\), причём равенство достигается тогда и только тогда, когда случайные величины независимы;
    \item \(H(\alpha) \le H(\alpha, \beta)\), причём равенство достигается тогда и только тогда, когда \(\beta\) полностью определяется значением \(\alpha\), 
    т.е. \(\beta = f(\alpha)\).
\end{itemize}
\end{lemma}
\begin{proof}
Введём обозначения для вероятностей событий совместного распределения вероятностей \((\alpha, \beta)\). Пусть пара \((a_i, b_j)\) имеет вероятность \(p_{i,j}\), событие \([\alpha=a_i]\) имеет вероятность \(p_{i,*} = p_{i,1} + \dotsb + p_{i,n}\), а событие 
\([\beta=b_j]\)~--- вероятность \(p_{*,j} = p_{1,j} + \dotsb + p_{k,j}\). В этих обозначениях неравенство \(H(\alpha, \beta) \le H(\alpha) + H(\beta)\) переписывается как
\[
\sum_{i,j}\cdot p_{i,j}\cdot \log\frac1{p_{i,j}} \le
\sum_{i}\sum_{j} p_{i,j}\cdot \log\frac1{p_{i,*}} +
\sum_{j}\sum_{i} p_{i,j}\cdot \log\frac1{p_{*,j}}.
\]
Перенесём всё в левую часть и применим неравенство Йенсена.
\begin{multline*}
\sum_{i,j}p_{i,j}\cdot \log\frac{p_{i,*}\cdot p_{*,j}}{p_{i,j}} \le
\log\left(\sum_{i,j}p_{i,j}\cdot \frac{p_{i,*}\cdot p_{*,j}}{p_{i,j}}\right) =
\log\left(\sum_{i,j}p_{i,*}\cdot p_{*,j}\right) = \\
= \log \underbrace{\left(\sum_{i}p_{i,*}\right)}_1\cdot
     \underbrace{\left(\sum_{j}p_{*,j}\right)}_1 = 0.
\end{multline*}

Равенство в неравенстве Йенсена для $f(x) = \log(x)$ достигается только, если все точки равны, т.е. 
для любых \(i,j\) \(\frac{p_{i,*}p_{*,j}}{p_{i,j}} = c\) для некоторой константы \(c\). Несложно заметить, что \(c =
1\), т.к. выполняется следующее равенство \(\sum_{i,j} {p_{i,*}p_{*,j}} = c \sum_{i,j}{p_{i,j}}\) в котором обе суммы
равны 1. Таким образом в случае равенства \(\alpha\) и \(\beta\) независимы.

Доказательство второго свойства мы получим как следствие из свойств условной энтропии.
\end{proof}

\begin{definition}\label{def:cond-entropy1}
Энтропия \(\alpha\) при условии \(\beta = b_j\)
\[H(\alpha\mid\beta = b_j) = \sum_i \Pr[\alpha = a_i\mid \beta = b_j]\cdot
    \log\frac{1}{\Pr[\alpha = a_i\mid \beta = b_j]}. \]
\end{definition}

\begin{definition}\label{def:cond-entropy2}
\emph{Условная (относительная) энтропия} \(\alpha\) относительно \(\beta\)
\[H(\alpha\mid\beta) = \sum_j \Pr[\beta = b_j] \cdot H(\alpha\mid \beta = b_j). \]
Другими словами
\[H(\alpha\mid\beta) = \mathop{\mathrm{E}}\limits_{b_j\gets\beta}[H(\alpha\mid \beta = b_j)]. \]
Если подставить определение~\ref{def:cond-entropy1}, то можно получить выражение для условной энтропии через отдельные вероятности событий.
\[H(\alpha\mid\beta) = 
\sum_j \Pr[\beta = b_j]\cdot 
\sum_i \Pr[\alpha = a_i\mid \beta = b_j]\cdot 
\log\frac{1}{\Pr[\alpha = a_i\mid \beta = b_j]}  = 
\sum_{i,j} p_{i,j}\cdot\log\frac{p_{*,j}}{p_{i,j}} . \]
\end{definition}
\begin{lemma}
Условная энтропия обладает следующими свойствами.
\begin{itemize}
    \item \(H(\alpha\mid\beta)\ge 0\).
    \item \(H(\alpha\mid\beta) = 0\) \(\iff\) \(\alpha\) однозначно определяется по \(\beta\). 
    \item \(H(\alpha,\beta) = H(\beta)  + H(\alpha\mid\beta) 
                            = H(\alpha) + H(\beta\mid\alpha)\).
\end{itemize}
\end{lemma}
\begin{proof}
Первое свойство выполняется, т.к. условная энтропия это матожидание неотрицательной случайной величины. Второе свойство объясняется тем, что для любого \(j\) распределение \(\langle\alpha\mid\beta=b_j\rangle\) имеет нулевую энтропию, т.е. распределение вырождено и каждому \(b_j\) соответствует ровно один \(a_i\). Третье свойство следует из следующего равенства.
\[
\sum_{i,j} p_{i,j} \cdot \log\frac{1}{p_{i,j}} =
\sum_{i,j} p_{i,j} \cdot \log\frac{1}{p_{*,j}} +
\sum_{i,j} p_{i,j} \cdot \log\frac{p_{*,j}}{p_{i,j}}.
\]
(Нужна аккуратность, если есть строки, которые состоят из одних нулей, т.е \(p_{*,j} = 0\)~--- такие строки не нужно включать в эти суммы.)
\end{proof}
\begin{corollary} \(H(\alpha,\beta) \ge H(\alpha)\), причём равенство достигается тогда и только тогда, когда \(\beta = f(\alpha) \).
\end{corollary}
\begin{proof}
\(H(\alpha,\beta) - H(\alpha) = H(\beta\mid\alpha) \ge 0\). По второму свойству условной энтропии равенство достигается тогда и только тогда, когда \(\beta = f(\alpha)\).
\end{proof}

\subsection{Взаимная информация}
\begin{definition}
\emph{Информация в \(\alpha\) о величине \(\beta\)} определяется следующим соотношением:
\[
    I(\alpha:\beta) = H(\beta) - H(\beta\mid\alpha).
\]
Эту величину так же \emph{называют взаимной информацией случайных величин \(\alpha\) и \(\beta\)}.
\end{definition}
\begin{lemma}
Для взаимной информации выполняются следующие соотношения.
\begin{enumerate}
\item \(I(\alpha:\beta) \le H(\alpha) \).
\item \(I(\alpha:\beta) \le H(\beta) \).
\item \(I(\alpha:\alpha) = H(\alpha) \).
\item \(I(\alpha:\beta) = I(\beta:\alpha) \).
\item \(I(\alpha:\beta) = H(\alpha) + H(\beta) - H(\alpha,\beta) \).
\end{enumerate}
\end{lemma}
\begin{definition}
    Пусть \(\alpha,\beta,\gamma\)~--- случайные величины. Определим
    \emph{взаимную информацию в \(\alpha\) о \(\beta\) при условии \(\gamma\)}.
    \begin{enumerate}
        \item \(I(\alpha:\beta\mid\gamma) = H(\beta\mid\gamma) -
            H(\beta\mid\alpha,\gamma).\)
        \item \(I(\alpha:\beta\mid\gamma) = \sum_\ell I(\alpha:\beta \mid
            \gamma=c_\ell)\cdot \Pr[\gamma = c_\ell].\)
        \item \(I(\alpha:\beta\mid\gamma) = H(\alpha\mid\gamma) + H(\beta\mid\gamma) -
            H(\alpha,\beta\mid\gamma).\)
        \item \(I(\alpha:\beta\mid\gamma) = H(\alpha,\gamma) + H(\beta,\gamma) -
            H(\alpha,\beta,\gamma) - H(\gamma)\).
    \end{enumerate}
\end{definition}
\begin{lemma} 
    Все определения условной взаимной информации эквивалентны.
\end{lemma}
\begin{proof}
    (3) \(\iff\) (4).
    \[
    (3) = H(\alpha\mid\gamma) + H(\beta\mid\gamma) - H(\alpha,\beta\mid\gamma) =
    H(\alpha,\gamma) - H(\gamma) + H(\beta,\gamma) - H(\gamma) -
    H(\alpha,\beta,\gamma) + H(\gamma).
    \]

\end{proof}

\subsection{Применение: опять о поиске фальшивой монетки}
Теперь у нас достаточно знаний, чтобы доказать, что за три взвешивания нельзя
найти одну фальшивую монету из 14, даже если не нужно определять её
относительный вес.
\begin{proof}
    Предположим, что существует способ найти фальшивую монету за три
    взвешивания. Тогда протокол взвешивания можно представить в виде полного
    троичного дерева, где каждый лист помечен номером монетки, которая оказалась
    фальшивой (у нас как раз ровно \(3^3=27\) исходов). 

    Давайте введём следующее распределение вероятностей \(\alpha\). 
    Пусть монета, номер которой находится в листе, соответствующем трём равенствам
    (такой лист только один), имеет номер \(i\). В нашем распределении
    вероятностей монета с номером \(i\) будет фальшивой с вероятностью \(1/27\).
    Оставшиеся монеты оказываются фальшивыми с вероятностями \(2/27\), причём
    с вероятностью \(1/27\) монета оказывается легче, чем настоящая, и с
    такой же вероятностью она оказывается тяжелее настоящей.
    \[
        H(\alpha) = \log 27 = 3\log 3.
    \]

    Пусть случайные величины \(\beta_1\), \(\beta_2\), \(\beta_3\) соответствуют
    результатам первого, второго и третьего взвешивания соответственно.
    Значение $\alpha$ однозначно определяется после трёх взвешиваний:
    \(
        H(\alpha\mid\beta_1,\beta_2,\beta_3) = 0,
    \)
    а следовательно
    \[
        H(\alpha) \le H(\beta_1,\beta_2,\beta_3) \le H(\beta_1) + H(\beta_2) +
        H(\beta_3) \le 3\log3.    
    \]
    Таким образом каждое взвешивание должно иметь энтропию ровно $\log3$.
    Рассмотрим первое взвешивание. Пусть на чашах весов лежит по $k$ монет.
    Вероятность каждого исхода взвешивания ($<$, $>$, $=$) относительно
    распределения $\alpha$ должна быть ровно $1/3$.
    \[
        \Pr[<] = \frac{k}{27} + \frac{k}{27} = \frac13.
    \]
    Таким образом $2k = 9$, а значит нет такого целого $k$.
\end{proof}
\begin{exercise}
    Пусть у нас есть $N$ камней разного веса и чашечные весы.
    Сколько нужно взвешиваний, чтобы найти 
    \begin{enumerate}
        \item самый тяжёлый и второй по тяжести камень,
        \item самый тяжёлый и самый лёгкий камни.
    \end{enumerate}

\end{exercise}

\section{Кодирование}
\subsection{Однозначно декодируемые коды}
\begin{definition}
    Будем называть \emph{кодом} функцию $C:\{\seqn{a}{n}\}\to\bitstr$,
    сопоставляющую буквам некоторого алфавита \emph{кодовые слова}.
    Если любое сообщение, которое получено применением кода $C$, декодируется
    однозначно (т.е. только единственным образом разрезается на образы $C$), 
    то такой код называется \emph{однозначно декодируемым}.
\end{definition}

\begin{definition}
    Код называется \emph{префиксным (беспрефиксным, prefix-free)}, если никакое
    кодовое слово не является префиксом другого кодового слова.
\end{definition}

\begin{theorem}[Неравенство Крафта-Макмилана]\label{thm:mcmill}
    Для любого однозначно декодируемого кода со множеством кодовых слов 
    \(\{\seqn{c}{n}\}\) выполняется следующее неравенство:
    \[
        \sum_{i=1}^{n} 2^{-|c_i|} \le 1.
    \]
\end{theorem}
\begin{lemma}
    Для префиксных кодов верно неравенство Крафта-Макмилана.
\end{lemma}
\begin{proof}
    Рассмотрим дерево префиксного кода и посчитаем суммарную меру поддеревьев,
    которые соответствуют кодовым словам.
\end{proof}

\begin{statement}
    Для префиксных кодов верно и обратное: если есть набор целых чисел
    \(\{\seqn{\ell}{n}\}\),
    удовлетворяющие неравенству Крафта-Макмилана
    \[
        \sum_{i=1}^{n} 2^{-\ell_i} \le 1,
    \]
    то существует префиксный код с кодовыми
    словами \(\{\seqn{c}{n}\}\), где \(|c_i| = \ell_i\).
\end{statement}
\begin{proof}
    Отсортируем $\ell_i$ по возрастанию и будем развешивать их в бесконечном
    двоичном дереве, выбирая каждый раз самый левый свободный узел
    соответствующей меры. Можно заметить, что мы всегда сможем найти такой узел.
\end{proof}
\begin{corollary}
    Для любого однозначно декодируемого кода существует префиксный код с теми же
    длинами кодовых слов.
\end{corollary}

\begin{proof}[Доказательства теоремы~\ref{thm:mcmill}]
    Сопоставим кодовым словам $\{c_i\}$ мономы $\{p_i\}$ от переменных $x$ и $y$ таким 
    образом, что каждый '$0$' в кодовом слове соответствует $x$, а каждая '$1$'~--- $y$:
    \[
        c_i = 0110101 \implies p_i(x,y) = xyyxyxy.
    \]
    Рассмотрим следующее выражение для некоторого $L$.
    \[
        \left( \sum_{i=1}^n p_i(x,y) \right)^L = \sum_{\ell=L}^{\max|c_i| \cdot
        L} M_\ell(x,y),
    \]
    где $M_\ell$ обозначает сумму всех получившихся одночленов степени $\ell$.
    Заметим, что в каждом $M_\ell$ не более $2^\ell$ одночленов: в противном
    случае код не был бы однозначно декодируемым~--- каждый одночлен (без учёта
    коммутативности и ассоциативности) мог получиться не более одного раза.
    
    Теперь рассмотрим значение этого выражения при \(x = y = \frac12\).
    \begin{equation}\label{eq:kraftproof}
        \left( \sum_{i=1}^n p_i\bigl(\sshalf,\sshalf\bigr) \right)^L =
        \sum_{\ell=L}^{\max|c_i| \cdot L} M_\ell\bigl(\sshalf,\sshalf\bigr) \le 
        \sum_{\ell=L}^{\max|c_i| \cdot L} (2^{-\ell} \cdot 2^\ell) \le 
        L\cdot\max|c_i| = O(L).
    \end{equation}

    Предположим теперь, что неравенство Крафта-Макмилана не выполняется, т.e.
    \[
        q = \sum_{i=1}^n p_i(1/2,1/2) = \sum_{i=1}^n 2^{-|c_i|} > 1.
    \]
    Сравнивая это с \eqref{eq:kraftproof} получаем противоречие:
    \(
        q^L = O(L)    
    \) 
    (левая часть растёт экспоненциально, а правая~— линейно).
\end{proof}

Пусть для каждого символа алфавита задана вероятность $p_i$. Нас будут
интересовать самые короткие в среднем коды, т.е. такие, что 
\[
        \sum_{i=1}^n p_i\cdot|c_i| \to \min. 
\]
\begin{theorem}[Шеннон]
    Для любого однозначно декодируемого кода выполняется
    \[
        \sum_{i=1}^n p_i\cdot|c_i|\ge \sum_{i=1}^n p_i\cdot \log\frac1{p_i}. 
    \]
\end{theorem}
\begin{proof}
    Перенесём всё в правую часть и применим неравенство Йенсена:
    \[
    \sum_{i=1}^n p_i\cdot\log\frac{2^{-|c_i|}}{p_i}\le 
    \log\sum_{i=1}^n \left(p_i\frac{2^{-|c_i|}}{p_i}\right) = 
    \log\sum_{i=1}^n 2^{-|c_i|} \le \log 1 = 0. 
    \]
\end{proof}

\begin{theorem}[Шеннон]\label{thm:shannon:optcode}
    Для любого распределения вероятностей $\{\seqn{p}{n}\}$ существует
    однозначно декодируемый/префиксный код $\{\seqn{c}{n}\}$, такой что
    \[
        \sum_{i=1}^n p_i\cdot|c_i|\le \sum_{i=1}^n p_i\cdot \log\frac1{p_i} + 1. 
    \]
\end{theorem}
\begin{remark}
    От '$+1$' в правой части никак не избавиться: например, если у нас только два символа в
    алфавите, то $\sum p_i\cdot|c_i| = 1$, в то время как $\sum
    p_i\log\frac{1}{p_i}$ может быть сколько угодно близко к нулю.
\end{remark}

\begin{proof}
    Покажем, что найдутся \(\{\seqn{c}{n}\}\) такие, что $|c_i| =
    \bigl\lceil\log\frac1p_i \bigr\rceil$. Код существует, т.к. для длин $c_i$ выполняется
    неравенство Крафта-Макмилана:
    \[
        \sum_{i=1}^n 2^{-|c_i|} =  
        \sum_{i=1}^n 2^{-\lceil\log\frac1p_i \rceil} \le  
        \sum_{i=1}^n 2^{-\log\frac1p_i} = 
        \sum_{i=1}^n p_i = 1. 
    \]
    Теперь оценим среднюю длину кода:
    \[
        \sum_{i=1}^n p_i\cdot |c_i| =  
        \sum_{i=1}^n p_i\cdot \bigl\lceil\log{\textstyle\frac1p_i} \bigr\rceil < 
        \sum_{i=1}^n p_i\cdot \bigl(\log{\textstyle\frac1p_i} + 1\bigr) =
        \left(\sum_{i=1}^n p_i\cdot \log{\textstyle\frac1p_i}\right) + 1.
    \]
\end{proof}
\subsection{Код Шеннона-Фано}
Упорядочим вероятности символов по убыванию: $p_1\ge p_2\ge\dotsb\ge p_n$.
Уложим на прямой без пропусков отрезки длиной $p_1$, $p_2$,\ldots, $p_n$
и обозначим $i$-ый отрезок через $S_i$, а их объединение~--- через $S$.
Коды тех букв $a_i$, для которых отрезок $S_i$ попал в левую половину $S$,
будут начинаться с '0', а коды тех букв, для которых отрезок $S_i$ попал
в правую часть $S$~--- с '1'. Центральный отрезок может не попасть
целиком в одну из половин $S$. Если центральный отрезок является первым
или последним, то начнём его код, соответственно, с '0' или '1'.
В противном случае отнесём его в произвольную половину $S$. Далее
применяем эту стратегию отдельно для букв из левой половины $S$ и отдельно для
правой половины $S$. Повторяем так пока не получим уникальные коды для всех
символов.     

\begin{definition}
    Будем называть кодирование, при котором для некоторой константы $c$ и для
    всех $i$ выполняется $|c_i|\le - \log p_i + c$, \emph{сбалансированным}.
\end{definition}

\begin{theorem}[Шеннон]
    Средняя длина кода Шеннона-Фано близка к энтропии, но не обязательно
    оптимальна:
    \[
        \sum_{i=1}^n p_i\cdot |c_i| = H + O(1). 
    \]
\end{theorem}
\subsection{Код Хаффмана}
\begin{definition}
    Будем строить \emph{код Хаффмана} по индукции. При $n = 2$ коды $c_1 =
    \langle0\rangle$, $c_2 = \langle1\rangle$. При $n > 2$ будем предполагать,
    что вероятности упорядочены по убыванию $p_1\ge p_2\ge\dotsb\ge p_n$.
    Заменим символы $a_{n-1}$ и $a_n$ на символ $a_{n-1}'$ с вероятностью
    $p_{n-1}' = p_{n-1} + p_n$. Построим код Хаффмана для $n-1$ символа.
    Для символов $a_{n-1}$ и $a_n$ возьмём коды $c_{n-1} = c_{n-1}'0$ и
    $c_{n} = c_{n-1}'1$.
\end{definition}
\begin{lemma}
    Средняя длина кодового слова для кода Хаффмана оптимальна, т.е. не превосходит средней длины
    любого другого префиксного кода (а значит и любого однозначно
    декодируемого).    
\end{lemma}
\begin{corollary}
    Для кода Хаффмана выполняется неравенство из теоремы Шеннона \ref{thm:shannon:optcode}.
\end{corollary}
\begin{remark}
    На энтропию случайной величины иногда удобно смотреть как на среднюю длину
    кода Хаффмена.
\end{remark}

\subsection{Блоковое кодирование}
Для того, чтобы нивелировать неустранимую '$+1$' в средней длине кода, мы будем
кодировать не отдельные символы, а блоки символов.
Пусть каждый блок состоит из $k$ символов. Пусть случайные величины $\seqn{\alpha}{k}$
распределены как $\alpha$ и соответствуют буквам в блоке. 
\[
    H(\seqn{\alpha}{k}) = \sum_{i=1}^k H(\alpha_i) = k\cdot H(\alpha).
\]
Тогда по теоремам Шеннона получается
следующее ограничение на среднюю длину кода символа в блоке:               
\[
    H(\alpha)\le (\text{средняя длина кода буквы в блоке})\le H(\alpha) +
    \frac{1}{k}.
\]
При кодировании блоков длины 100 мы получаем отклонение от энтропии не
более, чем на 0.01. Однако мы не можем применить код Хаффмена, т.к. на
вход алгоритму его построения нужно было бы передать $n^{100}$ частот
символов.

\subsection{Арифметическое кодирование}
Мы построим код со следующим ограничением на среднюю длину:
\[
    \sum_{i=1}^n p_i\cdot|c_i|\le \sum_{i=1}^n p_i\cdot \log\frac1{p_i} + 2, 
\]
что хуже, чем в теореме Шеннона.

\begin{definition}
    Будем называть полуинтервал \emph{стандартным}, если он имеет вид
    $[0.v0_2, 0.v1_2)$, где $v$~--- это некоторая последовательность битов,
    а числа записаны в двоичной системе счисления. Будем сопоставлять каждому
    стандартному интервалу $[0.v0_2, 0.v1_2)$ код $v$.

    Для первой буквы кода на отрезке [0,1] мы отложим слева направо непересекающиеся интервалы длины
    $p_i$. Пусть первая буква блока~--- это $a_{i_1}$, тогда для второй буквы кода мы внутри интервала
    соответствующего $p_{i_j}$ повторим эту операцию (отложим непересекающиеся интервалы), но длины интервалом 
    будут уже масштабированы с коэффициентом
    $p_i$. Повторим эту операцию $k$ раз. Получившемуся интервалу в качестве его кода 
    сопоставим код наибольшего стандартного интервала, который полностью содержится внутри него.
\end{definition}
\begin{statement}
    В интервале $[a,b)$ всегда найдётся стандартный интервал длины $2^{-k}$, где
    \(
    \frac{b-a}{4}<2^{-k}\le \frac{b-a}{2},
    \)
    т.е. длина кода любого интервала при арифметическом кодировании не
    превосходит $\log \frac{4}{b-a} = \log \frac{1}{p_i} + 2$.
\end{statement}

\begin{remark}
    В случае Марковской цепи можно строить код с соответствующими условными
    вероятностями.
\end{remark}
\begin{exercise}
Пусть Марковская цепь задана графом.
\begin{center}
\begin{tikzpicture}
\tikzstyle{vert}=[circle, draw, fill=black!50,
                        inner sep=0pt, minimum width=4pt]
\node[vert, label=below:0] (a) at (1,0) {}; 
\node[vert, label=below:1] (b) at (3,0) {};
\node (c) at (0,0) {};
\path
      (a) edge [->,bend left] node[label=above:$1-p$] {} (b)
      (b) edge [->,bend left] node[label=below:$1$]   {} (a)
      (a) edge [bend left, in=90] (0,0)
      (0,0) edge [->,bend left,out=90] (a);
\node[label=left:$p$] at (0,0) {};
\end{tikzpicture}
\end{center}
\[
h_p = \lim_{n\to\infty} \frac{H(\seqn{\alpha}{n})}{n}.
\]
Найти $\max\limits_p h_p$.
\end{exercise}

\subsection{Блоковые коды с ошибками}
Пусть $\seqn{\alpha}{n}$~--- независимые одинаково распределённые на
$\{\seqn{a}{k}\}$ случайные величины с вероятностями $\seqn{p}{k}$.
Рассмотрим блоковое кодирование, заданное функциями $E_n$ и $D_n$:
\[
    E_n:\{\seqn{a}{k}\}^n \to \{0,1\}^{L_n},
\]
\[
    D_n:\{0,1\}^{L_n} \to \{\seqn{a}{k}\}^n ,
\]
\begin{definition}
    \emph{Вероятность ошибки $\varepsilon_n$}~--- это вероятность следующего
    события: $[(\seqn{\alpha}{n}) = (\seqin{a}{i}{n}) \mid
    D_n(E_n(\seqin{a}{i}{n}))\neq (\seqin{a}{i}{n})]$.
\end{definition}
\begin{theorem}[Шеннон]\label{thm:blockcoding}
    При блоковом кодировании допускающем ошибки выполняются следующие
    соотношения.
    \begin{enumerate}
        \item Если $h > H(\alpha) = \sum_{i=1}^{k} p_i\log\frac{1}{p_i}$, то
            существует функции $(E_n, D_n)$ для $L_n = \lceil h\cdot n \rceil$,
            такие что $\varepsilon_n\to0$ при $n\to\infty$.

        \item Если $h < H(\alpha) = \sum_{i=1}^{k} p_i\log\frac{1}{p_i}$, то
            для любых функций $(E_n, D_n)$ для $L_n = \lceil h\cdot n \rceil$
            вероятность ошибки $\varepsilon_n\to1$ при $n\to\infty$.
    \end{enumerate}
\end{theorem}
\begin{definition}
    Будем называть слово $w=\langle\seqin{a}{i}{n}\rangle$ \emph{$\delta$-типичным}, если
    каждая буква $a_j$ встречается в нём $t_j$ раз, причём
    \[
    \begin{cases}
        t_j\le (p_j + \delta)\cdot n,\\
        t_j\ge (p_j - \delta)\cdot n.
    \end{cases}
    \]
\end{definition}
\begin{lemma}\label{lm:typicalwordprob}
    Для $\delta = n^{-0.49} = \frac{n^{0.01}}{\sqrt n}$ вероятность не
    $\delta$-типичного не превосходит $\varepsilon_n$, для $\varepsilon_n\to 0$.
\end{lemma}
\begin{proof}
    Применить неравенство Чебышева
    \[
    \mathrm {P}[|X-\mu |\ge \delta n]\le \frac {\sigma^2}{(\delta n)^2} =
    \frac{np_i(1-p_i)}{\delta^2 n^2} = O(n^{-0.02}). 
    \]
\end{proof}

\begin{lemma}\label{lm:typicalcount}
    Для $\delta = n^{-0.49}$ количество $\delta$-типичных слов не превосходит
    $2^{h\cdot n}$ (при достаточно больших $n$).
\end{lemma}
\begin{proof}
    Давайте для начала рассмотрим слова определённого \emph{типа}, в которых
    буква $i$ встречается $n_i$ раз, $n_1+n_2+\dotsb + n_k = n$. Сначала оценим
    количество слов типа, в котором $n_i = n\cdot p_i$. Таких слов
    \[
        \frac{n!}{n_1!n_2!\dotsm n_k!}.
    \]
    По формуле Стирлинга $n! \sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^n\cdot(1+o(1)).$
    \begin{multline}\label{eq:typicalwords}
        \log \frac{n!}{n_1!n_2!\dotsm n_k!} \approx 
        \log \frac{\poly(n) \left(\frac{n}{e}\right)^n}
            {\poly(n)\left(\frac{n_1}{e}\right)^{n_1}\dotsm
            \left(\frac{n_k}{e}\right)^{n_k}} = \\
        = \log \left(\frac{n}{n_1}\right)^{n_1}\dotsm
            \left(\frac{n}{n_k}\right)^{n_k} + O(\log n) 
        = \sum_{i=1}^k \underbrace{np_i}_{n_i}\cdot
            \log{\textstyle\frac{1}{p_i}} + O(\log n) < h\cdot n.
    \end{multline}
    Мы оценили это только для конкретного типа слов. Давайте оценим для
    произвольного $\delta$-типичного слова с $n_i = n\cdot(p_i + \Delta_i)$,
    где $|\Delta_i| \le \delta$. Тогда~\eqref{eq:typicalwords} изменится следующим образом:
    \[    
        \dots = 
        \sum_{i=1}^k n(p_i + \Delta_i)\cdot
            \log{\textstyle\frac{1}{p_i + \Delta_i}} + O(\log n) =         
        n\cdot \sum_{i=1}^k p_i\cdot
            \log{\textstyle\frac{1}{p_i}} + O(\log n) + n\cdot O(\delta) < h\cdot n. 
    \]
    (Действительно, энтропия~--- это непрерывная функция, а значит при небольшом 
    отклонении она изменяется на $c\cdot \max_i\Delta_i$, где $c$ зависит от производной функции энтропии.)
    Итого общее количество $\delta$-типичных слов можно оценить как количество
    типов умноженное на количество $\delta$-типичных слов одного типа:
    \[
        \poly(n) \cdot 2^{n\cdot H(\alpha) + n\cdot O(\delta) + O(\log n)} <
        2^{h\cdot n}.   
    \]
\end{proof}
\begin{proof}[Доказательство теоремы \ref{thm:blockcoding}] \mbox{}
    \begin{enumerate} 
    \item Если мы будем кодировать только $\delta$-типич\-ные слова, то по
    лемме~\ref{lm:typicalcount} нам будет достаточно длины кода $L_n$, а
    вероятность всех не типичных слов будет стремиться к нулю.

    \item Обозначим за $\hat\varepsilon_n$ вероятность ошибки при декодировании
         $\delta$-типичных слов.
        Мы хотим показать, что $\hat\varepsilon_n\to1$.  
        Давайте рассмотрим конкретное $\delta$-типичное слово
        $w=\langle\seqin{a}{i}{n}\rangle$. Пусть $p_1', p_2',\dotsc,p_n'$~---
        это частоты букв $\seqn{a}{n}$.
        Оценим вероятность появления $w$:
        \[
            \Pr[\langle\seqin{a}{i}{n}\rangle = w] = 
            p_1^{p_1'\cdot n} \cdot \dotsc \cdot p_k^{p_k'\cdot n} 
            =   2^{-(\sum_i p_i'\log\frac{1}{p_i})\cdot n}
            \le 2^{-(\sum_i p_i\log\frac{1}{p_i})\cdot n + O(\delta_n\cdot n)}.
        \]
        Всего мы может корректно закодировать не более $2^{L_n}$
        $\delta$-типичных слов, т.е. вероятность корректно декодировать
        $\delta$-типичное слово
        \[
            1 - \hat\varepsilon_n \le 2^{L_n}\cdot 2^{-H(\alpha)\cdot n + O(\delta_n\cdot n)} \le
            2^{h\cdot n + 1}\cdot 2^{-H(\alpha)\cdot n + O(\delta_n\cdot n)} 
            \to 0.
        \]
        Таким образом $\hat\varepsilon_n\to1$. Вместе с
        леммой~\ref{lm:typicalwordprob} получаем, что $\varepsilon_n\to1$.

    \end{enumerate}
\end{proof}
\begin{remark}
    Используя предыдущую теорему можно, например, получить альтернативное
    доказательство неравенства $H(\alpha,\beta)\le H(\alpha) + H(\beta).$ В
    левой части стоит асимптотическая средняя длина кода при блоковом
    кодировании $(\alpha,\beta)$, а справа сумма средних длин кодов
    при блоковом кодировании $\alpha$ и $\beta$ отдельно друг от друга. Т.к. мы
    можем рассмотреть кодирование $(\alpha,\beta)$ как конкатенацию кодов для
    $\alpha$ и $\beta$, то неравенство выполняется.
\end{remark}

\section{Свойства распределений}
\subsection{Энтропийные профили}
\begin{statement}
    Для любого $h\ge 0$ существует распределение $\alpha$: $H(\alpha) = h$. 
\end{statement}
\begin{proof}
    Возьмём некоторое целое $n$: $0\le h\le\log n$. Искомое распределение~---
    это линейная комбинация распределений с вероятностями $(1, 0, \dotsc, 0)$
    и $(\frac1n, \frac1n,\dotsc,\frac1n)$.
\end{proof}

Каким может быть совместное распределение двух случайных величин
$\alpha$ и $\beta$? Рассмотрим как может быть устроен \emph{энтропийный
профиль} $(H(\alpha), H(\beta), H(\alpha,\beta))$. 

\begin{statement}
    Для любых чисел $h_1, h_2, h_{12}\ge 0$, которые удовлетворяют следующим
    соотношениям
\[
    \left\{
    \begin{array}{lll}
        h_{12} \le h_1 + h_2 & \iff & t_0 = I(\alpha:\beta)\ge 0,\\
        h_{2} \le h_{12}     & \iff & t_1 = H(\alpha\mid\beta)\ge 0,\\
        h_{1} \le h_{12}     & \iff & t_2 = H(\beta\mid\alpha)\ge 0.
    \end{array}
    \right.
\]
    существует пара случайных величин $(\alpha,\beta)$ с энтропийным профилем $(h_1, h_2, h_{12})$.
\end{statement}
\begin{proof}
    Пусть $\xi_0,\xi_1,\xi_2$~--- независимые случайные величины с энтропиями
    $t_0,t_1,t_2$ соответственно. Тогда $\alpha=(\xi_0,\xi_1)$ и
    $\beta=(\xi_0,\xi_2)$ будут искомыми величинами.  
    \begin{center}
    
        \parbox{.4\textwidth}{
        \(
        \begin{cases}
            H(\xi_0) = t_0 = h_1 + h_2 - h_{12},\\
            H(\xi_1) = t_1 = h_{12} - h_{2},\\
            H(\xi_2) = t_2 = h_{12} - h_{1}.\\
        \end{cases}
        \)}
    \parbox{.4\textwidth}{\begin{tikzpicture}
    \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

    \node[circ, label=left:$\alpha$] (a) at (0,0) {}; 
    \node[circ, label=right:$\beta$] (b) at (1,0) {}; 
    \node at (-.4,0) {$\xi_1$};
    \node at (1.4,0) {$\xi_2$};
    \node at (.5,0)  {$\xi_0$};
    \end{tikzpicture}}
    \end{center}
\end{proof}

Давайте попробуем разобраться с аналогичным вопросом для троек случайных
величин. Энтропийный профиль для тройки $(\alpha,\beta,\gamma)$ будет задаваться 7 числами:
\[
\bigl(H(\alpha),H(\beta),H(\gamma),H(\alpha,\beta),H(\alpha,\gamma),
H(\beta,\gamma),H(\alpha,\beta,\gamma)\bigr).
\]
Для случайных величин $(\alpha,\beta,\gamma)$ можно записать 9 независимых
неравенств.
\begin{equation*}
\begin{array}{lll}
H(\alpha\mid\beta,\gamma)\ge 0, & I(\alpha:\beta )\ge 0, & I(\alpha:\beta\mid\gamma) \ge 0,\\
H(\beta\mid\gamma,\alpha)\ge 0, & I(\beta:\gamma )\ge 0, & I(\beta:\gamma\mid\alpha) \ge 0,\\
H(\gamma\mid\alpha,\beta)\ge 0, & I(\gamma:\alpha)\ge 0, & I(\gamma:\alpha\mid\beta) \ge 0.
\end{array}
\end{equation*}
\begin{definition}
Определим общую информацию трёх случайных величин
\[
    I(\alpha:\beta:\gamma) = I(\alpha:\beta) - I(\alpha:\beta\mid\gamma).
\]
\end{definition}

\begin{statement}
    Общая информация трёх случайных величин может быть отрицательной.
\end{statement}
\begin{proof}
    Пусть $\alpha$ и $\beta$ будут независимыми равномерно распределёнными на $\{0,1\}$ случайными
    величинами. Случайная величина $\gamma$ будет принимать значение из $\{0,1\}$ в соответствии со
    следующим соотношением:
    \[
        \alpha\oplus\beta\oplus\gamma = 0.
    \]
    Мы получим следующую картину:

    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$\alpha$]  (a) at (1,1) {}; 
        \node[circ, label=right:$\beta$]  (b) at (2,1) {}; 
        \node[circ, label=below:$\gamma$] (c) at (1.5,0) {}; 
        \node at (0.5,1.2) {$0$};
        \node at (1.5,-.4) {$0$};
        \node at (2.5,1.2) {$0$};

        \node at (0.9,0.4) {$1$};
        \node at (1.5,1.3) {$1$};
        \node at (2.1,0.4) {$1$};

        \node at (1.5,0.6) {$-1$};
    \end{tikzpicture}
    \end{center}

\end{proof}
\begin{statement}
    Других неравенств для троек нет.
\end{statement}
\begin{statement}
        Есть профили, которые не реализуются никакими распределениями, но их мера 0.
\end{statement}
\begin{exercise}
    Доказать, что следующий профиль реализуется только при $h=\log n$ для некоторого целого $n$.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$\alpha$]  (a) at (1,1) {}; 
        \node[circ, label=right:$\beta$]  (b) at (2,1) {}; 
        \node[circ, label=below:$\gamma$] (c) at (1.5,0) {}; 
        \node at (0.5,1.2) {$0$};
        \node at (1.5,-.4) {$0$};
        \node at (2.5,1.2) {$0$};

        \node at (0.9,0.4) {$h$};
        \node at (1.5,1.3) {$h$};
        \node at (2.1,0.4) {$h$};

        \node at (1.5,0.6) {$-h$};
    \end{tikzpicture}
    \end{center}
\end{exercise}
\begin{statement}
    \(2H(\alpha,\beta,\gamma)\le H(\alpha,\beta) + H(\alpha,\gamma) + H(\beta,\gamma)\).
\end{statement}
\begin{proof}
    Отметим сколько раз каждая область входит в левую/в правую часть неравенства.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=3cm]

        \node[circ, label=left:$\alpha$]  (a) at (1,1.3) {}; 
        \node[circ, label=right:$\beta$]  (b) at (2.4,1.3) {}; 
        \node[circ, label=below:$\gamma$] (c) at (1.7,0) {}; 
        \node at (0.2,1.5) {$2/2$};
        \node at (3.1,1.5) {$2/2$};
        \node at (1.7,-.6) {$2/2$};

        \node at (0.8,0.3) {$2/3$};
        \node at (2.6,0.3) {$2/3$};
        \node at (1.7,1.8) {$2/3$};

        \node at (1.7,0.8) {$2/3$};
    \end{tikzpicture}
    \end{center}
Таким образом утверждение упрощается до $0\le I(\beta:\gamma) + I(\alpha:\beta\mid\gamma) +
I(\alpha:\gamma\mid\beta).$
\end{proof}
\begin{corollary}[Теорема \ref{thm:volume}]
Для \(A\subset\bitstr\times\bitstr\times\bitstr\)
\[2\chi(A) \le \chi_{12}(A) + \chi_{13}(A) + \chi_{23}(A).\]
\end{corollary}
\begin{proof}
    Пусть $(\alpha,\beta,\gamma)$ равномерно распределены на $A$.
    \[
        2\chi(A) = 2H(\alpha,\beta,\gamma)\le 
        \underbrace{H(\alpha,\beta) }_{\le\chi_{12}(A)} + 
        \underbrace{H(\alpha,\gamma)}_{\le\chi_{13}(A)} + 
        \underbrace{H(\beta,\gamma) }_{\le\chi_{23}(A)}.
    \]
\end{proof}

\begin{statement}\label{st:someentropyineq}
    Для любых $\alpha$, $\beta$ и $\gamma$ выполняется следующее неравенство 
    \[
        H(\gamma)\le H(\gamma\mid\alpha) + H(\gamma\mid\beta) + I(\alpha:\beta).
    \]
\end{statement}
Если $H(\gamma\mid\alpha) = H(\gamma\mid\beta) = 0$ 
(т.е. $\gamma$ однозначно определяется и по $\alpha$ и по $\beta$), 
то $H(\gamma)\le I(\alpha:\beta)$.
\begin{proof}
    Отметим сколько раз каждая область входит в левую/в правую часть неравенства.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=3cm]

        \node[circ, label=left:$\alpha$]  (a) at (1,1.3) {}; 
        \node[circ, label=right:$\beta$]  (b) at (2.4,1.3) {}; 
        \node[circ, label=below:$\gamma$] (c) at (1.7,0) {}; 
        \node at (1.7,-.6) {$1/2$};
        \node at (1.7,1.8) {$0/1$};
        \node at (1.7,0.8) {$1/1$};

        \node at (0.8,0.3) {$1/1$};
        \node at (2.6,0.3) {$1/1$};

    \end{tikzpicture}
    \end{center}
    Таким образом неравенство упрощается до $0\le H(\gamma\mid\alpha,\beta) +
    I(\alpha:\beta\mid\gamma)$.
\end{proof}
\begin{exercise}
    Пусть $\alpha\to\beta\to\gamma$ образуют Марковскую цепь, т.е. распределение 
    $\langle\gamma\mid\beta\rangle = \langle\gamma\mid\alpha,\beta\rangle$.
    Докажите, что $I(\alpha:\gamma) \le I(\alpha:\beta)$ и $I(\alpha:\gamma) \le I(\beta:\gamma)$.
\end{exercise}
\begin{exercise}
    Пусть $\alpha\to\beta\to\gamma\to\delta$ образуют Марковскую цепь.
    Докажите, что $I(\alpha:\beta) \le I(\beta:\gamma)$.
\end{exercise}
\begin{exercise}\label{ex:existsdelta}
    Пусть $\alpha$, $\beta$ и $\gamma$ имеют следующий профиль.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$\alpha$]  (a) at (1,1) {}; 
        \node[circ, label=right:$\beta$]  (b) at (2,1) {}; 
        \node[circ, label=below:$\gamma$] (c) at (1.5,0) {}; 
        \node at (0.5,1.2) {$*$};
        \node at (1.5,-.4) {$*$};
        \node at (2.5,1.2) {$*$};

        \node at (0.9,0.4) {$0$};
        \node at (1.5,1.3) {$0$};
        \node at (2.1,0.4) {$0$};

        \node at (1.5,0.6) {$*$};
    \end{tikzpicture}
    \end{center}
    Докажите, что существует случайная величина $\delta$, такая что
    \[
        \begin{cases}
            H(\delta\mid\alpha) = 0,\\
            H(\delta\mid\beta)  = 0,\\
            H(\delta\mid\gamma) = 0,\\
            H(\delta) = I(\alpha:\beta:\gamma).
        \end{cases}
    \]
    И при этом $I(\alpha:\beta\mid\delta) = I(\alpha:\gamma\mid\delta) = I(\beta:\gamma\mid\delta) =
    0$.
\end{exercise}
\begin{exercise}
    Возьмём в качестве $x,y,a,b$ случайные величины из предыдущего упражнения: $x=\alpha$,
    $y=\beta$, $a=\gamma$, $b=\delta$. Покажите, что для любых таких $(a,b,x,y)$ из условия
    $I(x:y\mid a) = I(x:a\mid y) = I(y:a\mid x) = 0$ следует 
    \[
        I(a:b)\le I(a:b\mid x) + I(a:b\mid y) + I(x:y).     
    \]
    [Указание: примените неравенство из утверждения~\ref{st:someentropyineq}.]
\end{exercise}
\begin{exercise} 
    Возьмём в качестве $x,y,a,b$ случайные величины из упражнения~\ref{ex:existsdelta}: $x=\alpha$,
    $y=\beta$, $a=\gamma$, $b=\delta$. Покажите, что существуют такие $(a,b,x,y)$, для которых 
    \[
        I(a:b)\not\le I(a:b\mid x) + I(a:b\mid y) + I(x:y).     
    \]
    (Т.е. условие в предыдущем упражнении было необходимо.)
\end{exercise}
\begin{statement}[Неравенство для 5 случайных величин]
    \[
        I(a:b)\le I(a:b\mid x) + I(a:b\mid y) + I(x:y) 
        + I(a:b\mid z) + I(a:z\mid b) + I(b:z\mid a).
    \]
\end{statement}
\begin{corollary}[Zhang, Yeung, 1998]
    Неравенство для 4 случайных величин, которое не выражается через базисные неравенства.
    \[
        I(a:b)\le 2I(a:b\mid x) + I(a:b\mid y) + I(x:y) 
        + I(a:x\mid b) + I(b:x\mid a).
    \]
\end{corollary}
\begin{statement}
    Для 4 случайных величин существует бесконечно много неравенств, которые независимы в
    совокупности.
\end{statement}

\subsection{Неравенства о тройках}
Будем в различных предположениях доказывать следующее утверждение
\[
    H(a\mid x) + H(a\mid y) \le H(a).
\]

\begin{statement}
    Если $a,x,y$ такие, что
\[
    \begin{cases}
        H(a\mid y,x) = 0,\\
        I(x:y\mid a) = 0.
    \end{cases}
\]
то  \(H(a\mid x) + H(a\mid y) \le H(a)\).
\end{statement}
\begin{proof}
    Получается, что нам нужно доказать неотрицательность $h$. 
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$x$]  (x) at (1,1) {}; 
        \node[circ, label=right:$y$] (y) at (2,1) {}; 
        \node[circ, label=below:$a$] (a) at (1.5,0) {}; 
        \node at (1.5,-.4) {$0$};

        \node at (1.5,0.6) {$h$};
    \end{tikzpicture}
    \end{center}
Т.к. $I(x:y\mid a) = 0$, то $h = I(x:y)\ge 0$.
\end{proof}
\begin{statement}
    Если $a,x,y$ такие, что $H(a\mid y,x) = 0$ и
\[
    \begin{cases}
        A_i \sim X_j\\
        A_i \sim Y_k
    \end{cases} \implies A_i\sim(X_i,Y_k),
\]
то  \(H(a\mid x) + H(a\mid y) \le H(a)\). (Обозначение $A_i\sim X_j$ $\iff$ $\Pr[a=A_i \land x=X_j]>0$.)
\end{statement}
\begin{remark}
    Условие $H(a\mid x,y) = 0$ можно интерпретировать так: $a = f(x,y)$.
\end{remark}
\begin{proof}
    Построим новое распределение $(a',x',y')$:
    \begin{itemize}
        \item $a'$ имеет то же распределение, что и $a$,
        \item условное распределение $x'$ при условии $a'$ совпадает
            с условным распределением $x$ при условии $a$,
        \item условное распределение $y'$ при условии $a'$ совпадает
            с условным распределением $y$ при условии $a$,
        \item $x'$ и $y'$ независимы.
    \end{itemize}
    \[\
    \Pr[a'=A_i, x' = X_j, y' = Y_k] = 
    \Pr[a'=A_i]\cdot \Pr[x' = X_j\mid a'=A_i]\cdot \Pr[y' = Y_k\mid a' = A_i].
    \]
    Таким образом
    \[
        H(a',x',y') = H(a') + H(x'\mid a') + H(y'\mid a') + \underbrace{I(x':y'\mid a')}_{0}.
    \]
    С другой стороны
    \[
        H(a',x',y') \le H(x') + H(y') + H(a'\mid x',y').
    \]
    Кроме того, мы может стереть штрихи почти везде.
    \[
        H(x) + H(y) + H(a'\mid x',y') \ge H(a',x',y') = H(a) + H(x\mid a) + H(y\mid a).
    \]
    Покажем, что $H(a'\mid x', y') = 0$, т.е. $a' = f(x',y')$. Действительно: 
    если тройка $(A_i, X_i, Y_k)$ в новом распределении встречается с положительной
    вероятностью, то и в исходном распределении она так же встречалась с положительной
    вероятностью, следовательно $a' = f(x',y')$.
    Получаем: $H(a) + H(x\mid a) + H(y\mid a) \le H(x) + H(y)$. Прибавим $H(a)$ к обеим частям
    неравенства:
    \[
       H(x,a) + H(y,a) \le H(x) + H(y) + H(a)\implies H(a\mid x) + H(a \mid y) \le H(a).
    \]
\end{proof}
\begin{problem}[Верещагин]
    Рассмотрим двудольный граф с вершинами $(L,R)$ с цветными рёбрами.
    Все рёбра инцидентные одной вершине разноцветные, степень в левой доле не меньше $n$, 
    в правой~--- не меньше $m$. Пусть известно, что для пары вершин $(x\in L, y\in R)$
    есть не более одного общего цвета. Докажите, что количество цветов хотя бы $n\cdot m$.

    Заметим, что одноцветные рёбра образуют паросочетания. Для каждого цвета $c$ соединим все
    согласованные с $c$ вершины слева с согласованными с $c$ вершинами справа. Получим биклику из
    рёбер цвета $c$.

    Рассмотрим распределение на тройках $(a,x,y)$ (цвет, вершина из левой доли, вершина из правой
    доли): выбираем цвет пропорционально размеру соответствующей биклики и выбираем случайное ребро
    этого цвета. Можно проверить, что выполняется следующее соотношение:
\[
    \begin{cases}
        A_i \sim X_j,\\
        A_i \sim Y_k,
    \end{cases} \implies A_i\sim(X_i,Y_k).
\]
Теперь применим: $\underbrace{H(a\mid x)}_{\ge\log n} + 
                  \underbrace{H(a\mid y)}_{\ge\log m} \le H(a) \le \log (\text{\# цветов})$.
\end{problem}
\subsection{Условное неравенство о четвёрке}
\begin{statement}
    Если для случайных величин $a,b,x,y$ выполняется
    \[
        \begin{cases}
            I(x:y\mid a) = 0,\\
            H(a\mid x,y) = 0,
        \end{cases}
    \]
    то $I(a:b)\le I(a:b\mid x) + I(a:b\mid y) + I(x:y).$
\end{statement}
\begin{proof}
    Построим новое распределение $(a',b',x',y')$:
    сначала выберем значение $(a',b')\sim (a,b)$.
    При фиксированном значении $(a',b')$ выбираем независимо $x'$ и $y'$ так,
    чтобы условные распределения вероятностей относительно $a'$ были такими же, как у $x$ и $y$ относительно $a$.
    \begin{multline*}
    H(a',b',x',y') = H(a',b') + H(x\mid a',b') + H(y\mid a',b') - \underbrace{I(x':y'\mid
    a',b')}_{0} =\\
    = H(a,b) + H(x\mid a,b) + H(y\mid a,b).
    \end{multline*}
    С другой стороны
    \begin{multline*}
    H(a',b',x',y') \le H(b') + H(x'\mid b') + H(y'\mid b') + H(a'\mid x',y') =\\
    = H(b) + H(x\mid b) + H(y\mid b) + H(a'\mid x', y').
    \end{multline*}
    Покажем, что $H(a'\mid x',y') = 0$. В исходном распределении это выполнялось по условию. 
    Пусть $[a' = A_i, x' = X_j, y' = Y_k]$ в новом распределении случается с положительной
    вероятностью. Следовательно и в исходном распределении это случается с положительной 
    вероятностью (при фиксированном $a'$ величины $x'$ и $y'$ независимы), 
    а значит сохраняется соответствующее свойство функциональной зависимости $a'$ от $(x',y')$.

    В результате получаем
    \[
        H(a,b) + H(x\mid a,b) + H(y\mid a,b) \le H(b) + H(x\mid b) + H(y\mid b).
    \]
    Распишем это неравенство в безусловных энтропиях:
    \[
        H(a,b) + H(x,a,b) - H(a,b) + H(y, a,b) - H(a,b) \le H(b) + H(x, b) - H(b) + H(y, b) - H(b).
    \]
    Упрощаем и получаем:
    \begin{equation}\label{eq:condineq1}
        H(x,a,b) + H(y,a,b) + H(b) \le H(x,b) + H(y,b) + H(a,b).
    \end{equation}
    Проделаем то же самое с 
    $I(a:b)\le I(a:b\mid x) + I(a:b\mid y) + I(x:y).$
    \begin{equation*}
        \begin{array}{ll}
        H(a) + H(b) - H(a,b) \le 
        & H(a,x) + H(b,x) - H(a,b,x) - H(x) +\mbox{} \\
        & H(a,y)+ H(b,y) - H(a,b,y) - H(y) + \mbox{} \\
        & H(x) + H(y) - H(x,y).
        \end{array}
    \end{equation*}
    Упрощаем и получаем:
    \begin{equation}\label{eq:condineq2}
        \begin{array}{ll}
        H(a,b,x) + H(a,b,y) + H(b) + H(x,y) \le 
        & H(b,x) + H(b,y)  + H(a,b) + \mbox{} \\
        & H(a,x) + H(a,y) - H(a).
        \end{array}
    \end{equation}

    Заметим, что нам осталось доказать лишь $H(x,y)\le H(a) + H(x\mid a) + H(y\mid a)$. 
    Сложив это неравенство с~\eqref{eq:condineq1} мы получим \eqref{eq:condineq2}.
    Отметим сколько раз каждая область входит в левую/в правую часть неравенства.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=3cm]

        \node[circ, label=left:$x$]  (a) at (1,1.3) {}; 
        \node[circ, label=right:$y$] (b) at (2.4,1.3) {}; 
        \node[circ, label=below:$a$] (c) at (1.7,0) {}; 
        \node at (0.2,1.5) {$1/1$};
        \node at (3.1,1.5) {$1/1$};
        \node at (1.7,-.6) {$0/1$};

        \node at (0.8,0.3) {$1/1$};
        \node at (2.6,0.3) {$1/1$};
        \node at (1.7,1.8) {$1/2$};

        \node at (1.7,0.8) {$1/1$};
    \end{tikzpicture}
    \end{center}
    Т.е. оно эквивалентно $H(a\mid x,y) + I(x:y\mid a) \ge 0.$
\end{proof}
Вопросы на подумать. Придумать интерпретацию для этого неравенства. Zhang и Yeung в 97 году доказали это же
неравенство в предположении $I(x:y) = I(x:y\mid a) = 0$. Есть ли комбинаторная интерпретация у этого
утверждения?

\section{Криптография}
\subsection{Шифрования с закрытым ключом}

Рассмотрим задачу кодирования сообщения при помощи симметричного шифрования.
Будем считать, что вычислительные ресурсы противника неограниченны. 
Предположим, что мы шифруем сообщение $x$ с ключом шифрования $k$. При
шифровании сообщения мы получаем \emph{шифрограмму} $m = E(x, k)$.
Получатель шифрограммы тоже знает ключ $k$ и может узнать исходное
сообщение $x= D(m,k).$

Будем предполагать, что $x$ и $k$ являются случайными 
величинами. Противник не знает $x$ и $k$, но знает $m$. Для идеальной
схемы шифрования должны выполняться следующие соотношения:
\[
\begin{cases}
    H(m\mid x,k) = 0,\\
    H(x\mid m,k) = 0,\\
    I(m : x) = 0.
\end{cases}
\]

\begin{theorem}[Шеннон]
    $H(k)\ge H(m)$, даже если мы ослабим условие выкинув первое условие
    $H(m\mid x,k) = 0$ (т.е. разрешим алгоритму $E$ использовать случайные
    биты).
\end{theorem}     
\begin{remark}
    Одноразовый блокнот (one-time notepad) обладает этим свойством.
\end{remark}
\begin{proof}
    По условию $a + d = 0$, т.е. $d = -a$. 

    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$x$]  (a) at (1,1) {}; 
        \node[circ, label=right:$m$]  (b) at (2,1) {}; 
        \node[circ, label=below:$k$] (c) at (1.5,0) {}; 
        \node at (0.5,1.2) {$0$};
        \node at (1.5,-.4) {$c$};

        \node at (1.5,1.3) {$a$};
        \node at (2.1,0.4) {$b$};

        \node at (1.5,0.6) {$d$};
    \end{tikzpicture}
    \end{center}
    Т.к. взаимная информация неотрицательна, то $d + b\ge 0$, т.е. 
    $b \ge -d = a$. Теперь из $b \ge a$ и $c\ge 0$ следует $H(k)\ge H(x)$.                                  
\end{proof}


\subsection{Схемы разделения секрета}

Пусть у нас есть некоторый секрет $S_0$ и $n$ участников и мы хотим разделить между ними этот секрет
так, чтобы они могли им воспользоваться только все вместе, а любое подмножество участников~--- не
могло.
\begin{definition}
    \emph{Совершенная схема разделения секрета}~--- это совместное распределение вероятностей
    $(S_0,\seqn{S}{n})$, такое что
    \[
    \begin{cases}
        H(S_0\mid\seqn{S}{n}) = 0,\\
        H(S_0\mid\seqin{S}{i}{k}) = H(S_0), & k <  n.
    \end{cases}
    \]
    Второе условие можно переписать как $I(S_0:\seqin{S}{i}{k}) = 0$.
\end{definition}

Для совершенной схемы разделения секрета есть простая конструкция. Будем считать, что $S_0$
записан (закодирован) при помощи $\ell$ бит. Выберем независимо и равномерно
$S_1,\dotsc,S_{n-1}\in\{0,1\}^\ell$. $S_n$ определяется из условия 
$S_0 \oplus S_1 \oplus S_2 \oplus \dotsb \oplus S_n = \vec 0$ (покоординатная сумма по модулю 2).
\begin{statement}
    Предложенная схема разделения секрета является совершенной.
\end{statement}

\begin{definition}
    \emph{Пороговая совершенная схема разделения секрета}~--- это совместное распределение вероятностей
    $(S_0,\seqn{S}{n})$, такое что
    \[
    \begin{cases}
        H(S_0\mid\seqin{S}{i}{t}) = 0,\\
        H(S_0\mid\seqin{S}{i}{k}) = H(S_0), & k < t.
    \end{cases}
    \]
\end{definition}
\paragraph{Пороговая схема Шамира.} Будем считать, что секрет $S_0$~--- это элемент некоторого конечного
поля $\mathbb{F}_q$. Выберем случайный многочлен $p$ над полем $\mathbb{F}_q$ степени не более
$t-1$: выберем $t-1$ коэффициент независимо и равномерно, а последний (свободный) коэффициент определим из
соотношения $p(0) = S_0$. Выберем произвольным образом и сообщим всем участникам некоторый
набор различных ненулевых элементов поля $\seqn{a}{n}\in\mathbb{F}_q$ и вычислим секреты участников
как значение полинома в соответствующих точках $S_i = p(a_i)$. Теперь любые $t$ участником могут
собраться, воспользоваться формулой для интерполяции многочлена и вычислить $S_0=p(0)$. Если же
соберётся меньше участников, то у них не будет никакой информации об $S_0$.
\begin{statement}
    Пороговая схема Шамира является совершенной.
\end{statement}
\begin{proof}
    Любой полином степени меньше $t-1$ можно дополнить до полинома большей степени с любым значением в точке $0$.
\end{proof}

\begin{definition}
    \emph{Совершенная схема разделения секрета для структуры доступа $\Gamma\subset
    2^{[n]}$} ($\Gamma$ должно быть замкнуто вверх)~--- это совместное распределение вероятностей
    $(S_0,\seqn{S}{n})$, такое что
    \[
    \begin{cases}
        H(S_0\mid\seqin{S}{i}{m}) = 0,      & \{\seqn{i}{m}\}\in\Gamma,\\
        H(S_0\mid\seqin{S}{i}{m}) = H(S_0), & \{\seqn{i}{m}\}\not\in\Gamma.
    \end{cases}
    \]
\end{definition}
\begin{definition}
    \emph{Идеальная схема разделения секрета}~--- это
    совершенная схема разделения секрета с дополнительным требованием ,,экономности``.
    \[
    \forall i\in\{1,2,\dotsc,n\},\ H(S_i)\le H(S_0).
    \]
\end{definition}

\begin{statement}
        Если участник $i$ является \emph{существенным} в структуре доступа $\Gamma$ (т.е. существует
        такое $s\in\Gamma$, что $s\setminus\{i\}\not\in\Gamma$), то $H(S_i)\ge H(S_0)$. 
\end{statement}
\begin{remark}
    Схема Шамира является идеальной.
\end{remark}
\begin{proof} Пусть $s=\{i,\seqn{j}{k}\}\in\Gamma$, а $s\setminus \{i\}\not\in\Gamma$. 
    Обозначим взаимную информацию  $I(S_0:\seqin{S}{j}{k}\mid S_i)$ за $h$, 
    а $I(S_i:\seqin{S}{j}{k}\mid S_0)$ за $g$. Из условия 
    $I(S_0:\seqin{S}{j}{k}) = 0$ получаем, что $I(S_0:S_i:\seqin{S}{j}{k}) = -h$, аналогичным
    образом из $I(S_i:\seqin{S}{j}{k})\ge 0$ получаем, что $g \ge h$.

    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$S_i$]  (a) at (1,1) {}; 
        \node[circ, label=right:$S_0$]  (b) at (2,1) {}; 
        \node[circ, label=below:$\seqin{S}{j}{k}$] (c) at (1.5,0) {}; 
        \node at (0.5,1.2) {$\ge 0$};
        \node at (2.5,1.2) {$0$};

        \node at (2.1,0.4) {$h$};
        \node at (0.9,0.4) {$g$};

        \node at (1.5,0.6) {$-h$};
    \end{tikzpicture}
    \end{center}
    Таким образом $H(S_i) \ge H(S_0)$.
\end{proof}
\begin{remark}
    Это утверждение показывает, что не бывает более ,,экономной`` схемы разделения секрета, чем идеальная.
\end{remark}


\begin{statement}
    Для любой системы доступа $\Gamma$ существует совершенная схема разделения секрета.
\end{statement}
\begin{proof}
    Давайте для каждого подмножества $A = \{\seqn{i}{k}\}\in\Gamma$ создадим собственный набор секретов
    $S^A_{i_1}, S^A_{i_2},\dotsc,S^A_{i_k}$: $S^A_{i_1}\oplus S^A_{i_2}\oplus\dotsb\oplus S^A_{i_k}
    = S_0$. 
   (Достаточно рассматривать только минимальные множества $A$.)
\end{proof}
\begin{remark}
    Предложенная схема не является идеальной.
\end{remark}

\begin{statement}
    Существуют структуры доступа, для которых не существует идеальной схемы разделения секрета.
\end{statement}
\begin{proof}
    Рассмотрим структуру доступа, заданную следующим графом (рёбра соответствуют авторизованным
    множествам).
    \begin{center}
    \begin{tikzpicture}
    \tikzstyle{vert}=[circle, draw, fill=black!50,
                            inner sep=0pt, minimum width=4pt]
    \node[vert, label=below:1, label=above:\strut] (a) at (1,0) {}; 
    \node[vert, label=below:2] (b) at (2,0) {};
    \node[vert, label=below:3] (c) at (3,0) {};
    \node[vert, label=below:4] (d) at (4,0) {};

    \path (a) edge (b) edge (c) edge (d);
    \end{tikzpicture}
    \end{center}
    Покажем, что для этой структуры доступа $H(S_2) + H(S_3) \ge 3H(S_0)$, другими словами
    $\max_i\frac{H(S_i)}{H(S_0)} \ge 3/2$.

    Для доказательства нам потребуются три леммы. Будем обозначать $h = H(S_0)$.
    \begin{lemma}
        $H(S_2\mid S_1, S_3) \ge h$.
    \end{lemma}
    \begin{proof} Второй участник может восстановить секрет, воспользовавшись либо секретом первого
        или секретом третьего участника, т.е. $I(S_2 : S_0 \mid S_1, S_3) = h$.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$S_2$]  (a) at (1,1) {}; 
        \node[circ, label=right:$S_0$]  (b) at (2,1) {}; 
        \node[circ, label=below:{$S_1,S_3$}] (c) at (1.5,0) {}; 
        \node at (0.5,1.2) {$\ge 0$};
        \node at (2.5,1.2) {$0$};
        \node at (1.5,1.3) {$h$};
    \end{tikzpicture}
    \end{center}
    Таким образом $H(S_2 \mid S_0) \ge  I(S_2:S_0\mid S_1,S_3) = h$.
    \end{proof}

    \begin{lemma}
        $H(S_3\mid S_1) \ge h$.
    \end{lemma}
    \begin{proof} Аналогично предыдущей лемме получаем, что $H(S_3\mid S_1, S_4)\ge h$, и как
        следствие $H(S_3\mid S_1)\ge h$.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$S_3$]  (a) at (1,1) {}; 
        \node[circ, label=right:$S_0$]  (b) at (2,1) {}; 
        \node[circ, label=below:{$S_1,S_4$}] (c) at (1.5,0) {}; 
        \node at (0.5,1.2) {$\ge 0$};
        \node at (2.5,1.2) {$0$};
        \node at (1.5,1.3) {$h$};
    \end{tikzpicture}
    \end{center}
    \end{proof}

    \begin{lemma}\label{lm:secret4:l3}
        $I(S_1 : S_3\mid S_2) \ge h$.
    \end{lemma}
    \begin{proof} 
        Следующую схему следует интерпретировать как энтропия при условии $S_2$.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left: $S_1\mid S_2$]  (a) at (1,1) {}; 
        \node[circ, label=right:$S_3\mid S_2$]  (b) at (2,1) {}; 
        \node[circ, label=below:$S_0\mid S_2$] (c) at (1.5,0) {}; 

        \node at (0.9,0.4) {$0$};
        \node at (1.5,1.3) {$\ge0$};
        \node at (2.1,0.4) {$0$};

        \node at (1.5,-.4) {$0$};
        \node at (1.5,0.6) {$h$};
    \end{tikzpicture}
    \end{center}
    Заметим, что $I(S_1 : S_0 \mid S_2) = h$ и $I(S_3 : S_0 \mid S_2) = h$ в то время, 
    как $I(S_1 : S_0 \mid S_2, S_3) = 0$ и $I(S_3 : S_0 \mid S_1, S_2) = 0$.
    Т.е. $I(S_1:S_3:S_0\mid S_2) = h$, следовательно $I(S_1 : S_3\mid S_2) \ge h$.
    \end{proof}
    Теперь осталось сложить результаты трёх лемм: 
    \[
        H(S_2) + H(S_3)\ge H(S_2,S_3) = H(S_2\mid S_1,S_2) + H(S_3\mid S_1) + I(S_1:S_3\mid S_2) +
        I(S_2:S_1) \ge 3h.
    \]
\end{proof}
\begin{exercise}
    Доказать, что для любой схемы разделения секреты для этой структуры
    $\max_i\frac{H(S_i)}{H(S_0)} \ge 3/2$.
    \begin{center}
    \begin{tikzpicture}
    \tikzstyle{vert}=[circle, draw, fill=black!50,
                            inner sep=0pt, minimum width=4pt]
    \node[vert, label=below:1] (a) at (1,0) {}; 
    \node[vert, label=below:2] (b) at (2,0) {};
    \node[vert, label=below:3] (c) at (3,0) {};
    \node[vert, label=below:4] (d) at (4,0) {};

    \path (a) edge (b) edge (c) edge (d)
          (b) edge[bend left] (d);
    \end{tikzpicture}
    \end{center}
\end{exercise}
\begin{theorem}[Csirmaz'94]
    Существуют структуры доступа $\Gamma$ на $n$ участниках, такие что для любой схемы разделения
    секрета $\max_i\frac{H(S_i)}{H(S_0)} \ge \Omega(n/\log n)$.
\end{theorem}
\begin{proof}
    Выберем $n$ и $k$ такие, что $n = 2^k + k + 1$, и два множества участников 
    \[
        \begin{array}{l}
            A = \{\seqn{a}{k}\},\\
            B = \{\seqn{b}{2^k - 1}\}.
        \end{array}
    \]
    Для определения структуры доступа нам потребуются два семейства множеств. 
    Пусть $\{A_0,\seqn{A}{2^k-1}\}$~--- это все подмножества $A$, причём $A_0 = A$ и для любых
    $i < j$ выполняется $A_i\not\subseteq A_j$ (например, можно их упорядочить по уменьшению
    размера). Построим множества $\{B_0,\seqn{B}{2^k - 1}\}$ следующим образом: $B_0 = \emptyset$, 
    $B_i = \{\seqn{b}{i}\}$.  
    Теперь мы готовы определить структуру доступа $\Gamma$: $\Gamma = \{U_i\}_{i=0}^{2^k-1}$, где $U_i =
    A_i\cup B_i$. 
    
    Как и в предыдущих утверждениях обозначим $H(S_0)$ за $h$.
    В дальнейших рассуждениях мы будем использовать следующую нотацию: под энтропией некоторого
    множества участников $X = \{\seqn{x}{t}\}\subset A\cup B$, мы будем понимать энтропию секретов, которые
    принадлежат участникам этого множества, т.е. $H(X) = H(\seqin{S}{x}{t})$.

    \begin{lemma}\label{lm:secretlb} Для $i=\{0,1,2,\dots,2^k-2\}$
        \[
            H(A\cup B_i) - H(B_i) \ge H(A\cup B_{i+1}) - H(B_i+1) + h.
        \]
    \end{lemma}
    Из этой леммы следует, что 
    \begin{multline*}
        H(A) = H(A\cup B_0) - H(B_0) \ge H(A\cup B_{1}) - H(B_1) + h \ge \dotsb \ge\\
        \ge \underbrace{H(A\cup B_{2^k-1}) - H(B_{2^k-1})}_{\ge 0} + (2^k - 1)\cdot h.
    \end{multline*}
    Получаем, что $H(A) = H(\seqin{S}{a}{k})\ge (2^k - 1) \cdot h$. Следовательно есть $i$ такое,
    что $H(S_{a_i})\ge \frac{2^k - 1}{k}\cdot h$.
    Вспомним, что мы выбрали $n = 2^k + k + 1$, т.е. $H(S_{a_i})\ge \Omega(n/\log n) \cdot h$.
    Осталось доказать лемму.
    \begin{proof}[Доказательство леммы \ref{lm:secretlb}]
        Докажем два неравенства:
        \begin{enumerate}
            \item $H(A_{i+1}\cup B_i) + H(B_{i+1}) \ge  H(A_{i+1} \cup B_{i+1}) + H(B_i) $.
            \item $H(A\cup B_i) + H(A_{i+1}\cup B_{i+1}) \ge H(A\cup B_{i+1}) + H(A_{i+1}\cup B_i) + h$.
        \end{enumerate}
        Заметим, что если сложить эти два неравенства, то мы получим утверждение леммы.
    
        Первое неравенства говорит о неотрицательности условной совместной информации.
        Действительно, давайте вспомним формулу для условной совместной информации:
        \[
            I(x:y\mid z) \ge 0 \iff H(x,z) + H(y,z)\ge H(x,y,z) + H(z).
        \]
        Таким образом первое неравенство утверждает $I(A_{i+1}:\{b_{i+1}\}\mid B_i)\ge 0$.

        Аналогично второе неравенство утверждает, $I(A:\{b_{i+1}\}\mid A_{i+1}\cup B_i)\ge h$.
        Доказательство этого утверждения аналогично лемме~\ref{lm:secret4:l3}~--- нужно рассмотреть
        условное распределение при известном $A_{i+1}\cup B_i$.
    \begin{center}
    \begin{tikzpicture}
        \tikzstyle{circ}=[circle, draw, inner sep=0pt, minimum width=2cm]

        \node[circ, label=left:$A\mid A_{i+1}\cup B_i$]  (a) at (1,1) {}; 
        \node[circ, label=right:{$\{b_{i+1}\}\mid A_{i+1}\cup B_i$}]  (b) at (2,1) {}; 
        \node[circ, label=below:{$S_0\mid A_{i+1}\cup B_i$}] (c) at (1.5,0) {}; 
        \node at (0.9,0.4) {$0$};
        \node at (1.5,1.3) {$\ge0$};
        \node at (2.1,0.4) {$0$};

        \node at (1.5,-.4) {$0$};
        \node at (1.5,0.6) {$h$};
    \end{tikzpicture}
    \end{center}
    \end{proof}
    Эта лемма завершает доказательство теоремы.
\end{proof}
    \begin{remark}
        Нижние оценки на избыточную сложность совершенных схем разделения
        секрета влекут нижние оценки на схемную сложность монотонных функций.
    \end{remark}

\section{Коммуникационная сложность}
Пусть $X$, $Y$ и $Z$~--- это три конечных множества, и пусть задана некоторая функция $f:X\times Y \to Z$.
Два игрока, будем называть их Алиса и Боб, решают \emph{коммуникационную задачу для функции $f$}, если:
\begin{enumerate}
    \item множества $X$, $Y$, $Z$ и функция $f$ известны обоим игрокам,
    \item Алиса знает некоторое $x\in X$,
    \item Боб знает некоторое $y\in Y$,
    \item Алиса и Боб стремятся вычислить $f(x,y)$.
\end{enumerate}
Для решения этой коммуникационной задачи Алиса и Боб могут пересылать друг другу сообщения.
Задача считается решённой, если оба игрока знают $f(x,y)$.
Нас интересует минимальное количество битов, которое необходимо и достаточно переслать
для вычисления $f(x,y)$.

\begin{definition}
\emph{Коммуникационный протокол} для функции $f: X\times Y \to Z$~--- это корневое
двоичное дерево, которое описывает совместное вычисление Алисой и Бобом функции $f$.
В этом дереве каждая внутренняя вершина $v$ помечена меткой А или Б,
означающей очередь хода Алисы или Боба соответственно.
Для каждой вершины, помеченной А, определена функция $g_v: X \to \bits$, 
которая говорит Алисе, какой бит нужно послать,
если вычисление находится в этой вершине. Аналогично, для каждой вершины $v$ 
с пометкой Б определена функция $h_v: Y\to \bits$, которая определяет бит, 
который Боб должен отослать в этой вершине. Каждая внутренняя вершина имеет двух
потомков, ребро к первому потомку помечено $0$, а ребро ко второму потомку
помечено $1$. Каждый лист помечен значением из множества $Z$.

Вычисление по такому протоколу на конкретной паре входов $(x,y)$ устроено так:
изначально вычисление находится в корне. В каждой внутренней вершине $v$ в 
зависимости от пометки либо Алиса, либо Боб пересылают один бит 
(он определяется соответствующей функцией $g_v$ или $h_v$). После этого
вычисление переходит в один из потомков вершины $v$ по ребру, пометка которого
совпадает с битом, переданным в вершине $v$. Когда вычисление приходит в лист,
то оно завершается. Результат вычисления~--- это пометка в листе.
\end{definition}

Будем говорить, что коммуникационный протокол \emph{вычисляет функцию $f$}, если
для всех пар $(x,y)\in X\times Y$ вычисление приходит в лист с пометкой $f(x,y)$.
Теперь можно дать формальное определение \emph{коммуникационной сложности функции $f$}.

Аналогичным образом можно определить \emph{коммуникационный протокол, вычисляющий отношение $R \subset (X\times
Y)\times Z$}~--- нужно только дополнительно потребовать, чтобы ответы Алисы и Боба были согласованы.

\begin{definition} 
    \emph{Коммуникационная сложность} функции $f$ определяется как наименьшая глубина
    протокола (максимальная рёберная длина пути от корня до листа), вычисляющего функцию $f$.
    Обозначается $D(f)$.
\end{definition}

\begin{statement}
        Для любой $f: \{0,1\}^n \times\{0,1\}^n \to \bits$, $D(f) \le n + 1$.
\end{statement}
\begin{proof}
        Алиса посылает Бобу свой вход, а Боб посылает Алисе значение $f$.
\end{proof}

\begin{example} Примеры функций с нетривиальной верхней оценкой на коммуникационную сложность.
    \begin{enumerate}
        \item (Pointer Chaising) $D(\mathrm{PC})\le k\log n$, 
            где $PC(x,y) = \underbrace{x(y(x(y(x(y(x(y(x}_{\text{$k$ раундов}}(0)))))))))$.
            У игроков есть двудольный ориентированный граф на $2n$ вершинах, у которого исходящая
            степень каждой вершины равна 1. Алиса знает левую долю, Боб~--- правую.
            В начале они кладут фишку на вершину с номером 0 из доли Алисы и начинают
            передвигать её по рёбрам. Всего они должны сделать  $k$ переходов по рёбрам графа.
            Ответ~--- номер финальной вершины.


        \item $D(\mathrm{MED}) = O(\log^2 n)$, где $x$ и $y$ интерпретируются как характеристические
            функции подмножеств $[n]$, а $\mathrm{MED}(x,y)$~--- медиана их объединения.
        \item $D(CIS_G) = O(\log^2 n)$, где $x$ интерпретируется как характеристическая
            функция некоторой клики в графе $G$, а $y$~--- как характеристическая 
            функция некоторого независимого множества в графе $G$. $\mathrm{CIS}(x,y) = 1$, 
            если клика и независимое множество имеют общую вершину.
            (Замечание: не известно графов $G$, для которых нельзя решить 
            эту задачу за $O(\log n)$.)
    \end{enumerate}
\end{example}

\subsection{Нижние оценки}
    Рассмотрим коммуникационный протокол для некоторой функции $f: X\times Y\to Z$. 
    Для каждой вершины $v$ определим множество $R_v\subset X\times Y$~--- множество
    всех пар $(x,y)\in X\times Y$, для которых вычисление приходит в вершину $v$.
\begin{statement}
    Для всех вершин $v$ множество $R_v$ является комбинаторным прямоугольником,
    т.е. существуют такие $X_v\subset X$ и $Y_v\subset Y$, что $R_v = X_v\times Y_v$.
\end{statement}
\begin{proof}
    Покажем по индукции. Это верно для корня. Если это верно для какой-то вершины $v$
    с пометкой А: $R_v = X_v\times Y_v$. Если Алиса пересылает бит $b$ и вычисление
    переходит в вершину $u$, то $R_u = X_u\times Y_u$, 
    где $X_u = \{x\in X_v\mid g_v(x) = b\}$, а $Y_u = Y_v$. Аналогично, если Боб 
    посылает бит $b$ и вычисление переходит в вершину $u$, то $R_u = X_u\times Y_u$, 
    где $X_u = X_v$, а $Y_u = \{y\in Y_v\mid h_v(y) = b\}$.
\end{proof}
\begin{corollary}
    Листья коммуникационного протокола для функции $f$ задают разбиение множества $X\times Y$ на 
    одноцветные прямоугольники.
\end{corollary}

Будем обозначать $C^R(f)$~--- минимальное количество \emph{одноцветных} прямоугольников, покрывающих $X\times Y$.

\begin{statement}
    $D(f) \ge \log C^R(f)$.
\end{statement}
\begin{proof}
    $D(f) \ge \log (\text{\# листьев}) \ge \log C^R(f)$.
\end{proof}


\paragraph{Метод размера прямоугольников.} Определим некоторую весовую функцию на элементах $X\times Y$.
Тогда верна следующая оценка \[
    C^R(f) \ge \frac{w(X\times Y)}{\max\limits_{\text{одноцв. } A\times B} w(A\times B)}. 
\]


\paragraph{Метод трудного множества (fooling set).} Это частный случай метода размера прямоугольников, 
при котором фиксируется некоторое множество $F\subset X\times Y$, а $w(x,y)$ определяется следующим образом:
\[
    w(x,y) = 
    \begin{cases}
        1, & (x,y)\in F,\\
        0, & (x,y)\not\in F.
    \end{cases}
\]
При этом никакой прямоугольник не содержит более одного элемента из $F$. Следовательно $C^R(f) \ge |F|$.

\paragraph{Метод ранга матрицы.} Рассмотрим \emph{матрицу функции $f$}~--- матрицу, в которой
строки индексированы элементами $X$, столбцы~--- элементами $Y$, а в ячейке $(x,y)$ стоит $f(x,y)$.
Если мы рассмотрим эту матрицу функции как матрицу $M$ над некоторым довольно большим полем, 
то можно показать, что $C^R(f)\ge \rank M$.
\begin{exercise}
    Докажите предыдущие утверждения.
\end{exercise}

\begin{statement}
    $D(\mathrm{EQ}) = n + 1$, где $\mathrm{EQ}(x,y) = 1 \iff x = y$.
\end{statement}

\begin{statement}
    $D(\mathrm{GE}) = n + 1$, где $\mathrm{GE}(x,y) = 1 \iff x \ge y$.
\end{statement}

\subsection{Связь протоколов и формул}
\begin{definition}  
    \emph{Игра Карчмера-Вигдерсона для функции $f : \{0,1\}^n \to \{0,1\}$}~--- это 
    следующая коммуникационная игра: Алиса получает $x\in f^{-1}(0)$, Боб получает 
    $y\in f^{-1}(1)$, и они вместе пытаются найте такое $i\in [n]$, что $x_i \neq y_i$. 
    Другими словами, игра Карчмера-Вигдерсона~--- это коммуникационная задача для 
    \emph{отношения} $$R_f = \{((x,y),i)\mid x\in f^{-1}(0), y\in f^{-1}(1), x_i\neq y_i\}.$$
    Отношение $R_f$ будем называть \emph{отношением Карчмера-Вигдерсона} для функции $f$.
\end{definition}

\begin{definition}
    \emph{Формула в базисе Де Моргана для функции $f:\{0,1\}^n \to \{0,1\}$}~--- это булевая 
    формула с переменными $\{\seqn{x}{n}\}$, соответствующим отдельным битам входа $f$,
    и со связками $\{\land, \lor, \neg\}$, вычисляющая функцию $f$.
    Законы Де Моргана позволяют нам предполагать, что все $\neg$ находятся непосредственно
    перед переменными. Заметим, что структура формулы Де Моргана представляет собой корневое дерево
    (листья соответствуют переменным, а внутренние вершина~--- логическим связкам).
\end{definition}

Будем называть \emph{формульной сложностью} $L(f)$ функции $f : \{0,1\}^n \to \{0,1\}$ ~---
это размер (количество вхождений переменных) минимальной формулы вычисляющей $f$. Если
говорить более формально, то нужно говорить не о конкретной функции, а о последовательности
функций.

\begin{definition}
    Для функции $f:\bitstr\to\{0,1\}$ определим последовательность функций $\{\seqn{f}{n},\dotsc\}$,
    где $f_i: \{0,1\}^i\to\{0,1\}$ и $\forall x\in\{0,1\}^i, f(x) = f_i(x)$. Тогда формульная сложность
    $L(f)$ функции $f$ ограничена $g(n)$, если для любого $n$ существует формула $\phi_n$ размера
    не более $g(n)$, вычисляющая функцию $f_n$.
\end{definition}

\begin{theorem}[Шеннон]
    Существует $f$: $L(f) = \Omega(2^n/n)$.
\end{theorem}
\begin{proof}
    Посчитаем количество формул размера не более $s$ (будем предполагать, что $s\ge n)$: 
    это количество можно оценить сверху как $(4\cdot s)^s$. В то же время число всех функций 
    $f: \{0,1\}^n \to \{0,1\}$ ровно $2^{2^n}$. Какого $s$ достаточно,
    чтобы вычислить все функции на $n$ битах?
    $$(4\cdot s)^s \ge 2^{2^n} \implies s\cdot \log(4\cdot s) \ge 2^n \implies s = \Omega(2^n/n).$$
\end{proof}
\begin{remark}
    Этот подсчёт показывает, что существуют функции с экспоненциальной формульной сложностью.
    Более того, любая случайная функция с большой вероятностью имеет такую сложность.
\end{remark}

Однако не известно \emph{явных} функция большой сложности. Лучшая известная на данный момент
нижняя оценка на формульную сложность явной функции это $\Omega(n^3)$ 
(оценка для функции Андреева, доказана Хостадом).

\begin{theorem}[Карчмер-Вигдерсон]
    Для каждой формулы $\phi$ вычисляющей $f$, существует такой
    протокол $\Pi_\phi$ для отношения Карчмера-Вигдерсона $R_f$, что его дерево
    совпадает с деревом, описывающим структуру формулу $\phi$. 
    Верно и обратное утверждение: если есть протокол для $R_f$, 
    то есть и формула с такой же структурой.
\end{theorem}
\begin{proof}
    Ход Алисы будет соответствовать связке $\land$,
    ход Боба~--- связке $\lor$. 

    \begin{itemize}
        \item \textbf{формула $\to$ протокол}\\
            Каждая внутренняя вершина протокола соответствует некоторой
            подформуле исходной формулы $\phi$. Будем поддерживать следующий инвариант: пусть $\phi_v$~--- 
            подформула, соответствующая текущей вершине протокола $v$, тогда $\phi_v(x) = 0$, а $\phi_v(y) = 1$.
            Это верно для начальной вершины (т.к. верно для $\phi$). Если для текущей вершины это верно, и
            $\phi_v = \phi_{v0} \land \phi_{v1}$, то Алиса пересылает бит $b$ такой, что
            $\phi_{vb}(x) = 0$ (такой бит должен быть по свойствам $\land$, т.к. $\phi_v(x) = 0$). 
            При этом мы знаем, что $\phi_v(y) = \phi_{v0}(y) = \phi_{v1}(y) = 1$, т.е. инвариант сохраняется.
            Аналогично, если $\phi_v = \phi_{v0} \lor \phi_{v1}$, то Боб пересылает бит $b$ такой, 
            что $\phi_{vb}(y) = 1$ (мы соответственно знаем, что $\phi_v(x) = \phi_{v0}(x) = \phi_{v1}(x) = 1$).
            Когда Алиса и Боб придут в некоторый лист, то по индукции получается, что значение в этом 
            листе на входе Алисы отличается от значения в листе на входе Боба, а значит номер переменной
            в листе соответствует номеру бита различия.

        \item \textbf{протокол $\to$ формула}\\
            Будем последовательно строить формулы для внутренних вершин протокола от листьев к корню.
            При этом будем поддерживать следующий инвариант: пусть $v$~--- вершина протокола, $X_v\times Y_v$~---
            соответствующий прямоугольник, тогда формула $\phi_v$ для вершины $v$ такая, что для всех $x\in X_v$,
            $\phi_v(x) = 0$ и для всех $y\in Y_v$, $\phi_v(y) = 1$. Пусть мы построили формулы $\phi_{v0}$ и
            $\phi_{v1}$ для сыновей некоторой вершины $v$. Если вершина $v$ соответствовала ходу Алисы,
            то для всех входов Алисы из множества $X_v$ формула $\phi_v$ должна быть равна 0. При 
            этом по индукционному предположению мы знаем, что для некоторых входов Алисы (на которых Алиса 
            посылает 0) $\phi_{v0}=0$, а для остальных обязательно $\phi_{v1} = 0$. С другой стороны для всех 
            входов Боба $y\in Y_v$, $\phi_{v0}(y) = \phi_{v1}(y) = 1$. Поэтому, если мы положим 
            $\phi_v = \phi_{v0} \land \phi_{v1}$, то инвариант сохранится. Аналогично, если вершина
            соответствовала ходу Боба, то следует положить $\phi_v = \phi_{v0} \lor \phi_{v1}$.

            Осталось объяснить, что мы будем делать с листьями. Заметим, что если в листе протокола
            написан некоторый индекс $i$, то в него могут попадать либо пары входов, для которых 
            ($x_i = 0$, $y_i=1$), либо входы, для которых ($x_i=1$, $y_i=0$), но не могут попадать
            одновременно. В противном случае можно было бы воспользоваться свойствами 
            комбинаторных прямоугольников и дать Алисе и Бобу входы с одинаковыми $i$-ми битами,
            которые привели бы в этот же лист.
            $$\begin{cases}
                (x,y)\in R_\ell, & x_i = 0, y_i = 1,\\
                (x',y')\in R_\ell, & x'_i = 1, y'_i = 0.
              \end{cases} \implies (x', y) \in R_\ell.
            $$
            Таким образом можно считать, что в каждом листе кроме номера бита различия записаны
            также значения этого бита у Алисы и у Боба. Если в листе $\ell$ с номером бита различия $i$
            записаны ($x_i = 0$, $y_i = 1$), то $\phi_\ell = x_i$, в обратном случае
            $\phi_\ell = \neg x_i$. 
    \end{itemize}
\end{proof}

Таким образом мы получили взаимно однозначное соответствие между протоколами и формулами.
Проблема в том, что сложность протоколов мы до этого измеряли в терминах максимальной 
глубины, а сложность формул~--- в терминах количества листьев. Давайте определим сложность
протокола в терминах количества листьев.

\begin{definition}
    Для отношения $R_f$ будем обозначать через $L(R_f)$ минимальное количество
    листьев в коммуникационном протоколе для $R_f$.
\end{definition}

\begin{corollary}
    Для любой функции $f$, $L(f) = L(R_f)$.
\end{corollary}

%С некоторыми потерями можно связать минимальный размер формулы для $f$ с минимальной глубиной формулы для $f$.
%\begin{statement}
%    Для любой $\alpha > 1$  и для любой формулы $\phi$ размера $s$ существует эквивалентная
%    формула $\phi'$ размера $s^\alpha$ и глубины $O(\log s)$ (константа зависит от $\alpha$).
%\end{statement}
%\begin{proof}
%    Определим рекурсивный алгоритм $A(\phi)$:
%    найдём в $\phi$ подформулу $\psi$ размера от $s/3$ до $2s/3$. 
%    Вернём $\phi' = (A(\psi) \land A(\phi|_{\psi = 1}))\lor(\neg A(\psi) \land A(\phi|_{\psi = 0})).$
%    Глубина рекурсии получится $\log_{3/2}(s)$, на каждой итерации глубина увеличивается на два.
%    Суммарная глубина $2\cdot \log_{3/2}(s)$. Таким образом размер формулы $\phi'$ не более
%    $2^{2\cdot\log_{3/2}(s)} = O(s^4).$
%\end{proof}

\begin{definition}
    Пусть $\mu$ это некоторое распределение на входах Алисы и Боба,
    а $X$, $Y$~--- соответствующие случайные величины.
    \emph{Внешнее информационное разглашение} протокола $\Pi$ на распределении $\mu$:
    $$\IC_\mu^{ext}(\Pi) = I(\Pi(X,Y) : X, Y).$$
    \emph{Внутреннее информационное разглашение} протокола $\Pi$ на распределении $\mu$:
    $$\IC_\mu^{int}(\Pi) = I(\Pi(X,Y) : X \mid  Y) + I(\Pi(X,Y) : Y \mid X).$$
\end{definition}

\begin{lemma}
    Для любого протокола $\Pi$ и любого распределения $\mu$
    \[
    D(\Pi) \ge \IC_\mu^{ext}(\Pi) \ge \IC_\mu^{int}.
    \]
\end{lemma}
\begin{proof}
    Первое неравенство тривиально (нельзя раскрыть больше информации, 
    чем количество переданных битов).
    Второе неравенство доказывается по индукции по количеству
    переданных бит. Пусть $T=\Pi(X,Y)$. База
    \[
        I(\emptyset:X,Y) = I(\emptyset:X\mid Y) + I(\emptyset:Y\mid X).
    \]
    Пусть бит номер $i$ передаёт Алиса. Пусть $T_i$~--- это первые $i$ битов,
    а $b_i$~--- бит пересылаемый Алисой. Тогда $b_i$ полностью определяется по $T_{i-1}$ и $X$.
    \[
        \begin{aligned}
        I(T_i:X,Y) 
        & = I(T_{i-1}:X,Y) + I(b_i:X,Y \mid T_{i-1}) \\
        & = I(T_{i-1}:X,Y) + I(b_i : Y\mid T_{i-1}) + I(b_i : X\mid Y, T_{i-1})\\
        & \ge I(T_{i-1}:X,Y) + I(b_i : X\mid Y, T_{i-1})\\
        & \ge I(T_{i-1}:X\mid Y) + I(T_{i-1}:Y\mid X) + I(b_i : X\mid Y, T_{i-1}) \\
        & = I(T_{i-1}:X\mid Y) + I(T_{i-1}:Y\mid X) + I(b_i : X\mid Y, T_{i-1}) + \underbrace{I(b_i : Y \mid X, T_{i-1})}_0 \\
        & = I(T_i:X\mid Y) + I(T_i:Y\mid X).
        \end{aligned}
    \]
\end{proof}

\begin{theorem}
    Пусть $\Pi$ коммуникационный протокол. Для любого распределения $\mu$:
    \(
        \log L(\Pi) \ge \IC_\mu^{ext}(\Pi).
    \)
    Кроме того существует такое распределение $\mu^*$ для которого 
    \(
    \log L(\Pi) = \IC_{\mu^*}^{ext}(\Pi).
    \)
    Будем называть $\mu*$ \emph{труднейшим} распределением для $\Pi$.
\end{theorem}
\begin{proof} Для детерминированных протоколов $\IC^{ext}(\Pi) = H_\mu(\Pi).$
    Первое утверждение теоремы следует из верхней оценки на этнропию (энтропия
    случайной величины не превосходит логарифм числа исходов):
    \[
    \IC_\mu^{ext}(\Pi) = H_\mu(\Pi) \le \log L(\Pi).
    \]

    Для доказательства второго утверждения мы предъявим распределение $\mu^*$:
    выберем (равномерно) случайный лист $l$ протокола $\Pi$ и в соответствующем 
    прямоугольнике $R_l$ выберем произвольную пару $(x,y)$. Полученное
    распределение $\mu^*$ равномерно на листьях $\Pi$, поэтому
    \[
    \IC_{\mu^*}^{ext}(\Pi) = H_{\mu^*}(\Pi) = \log L(\Pi).
    \]
\end{proof}
\begin{corollary}
    Пусть $f$~--- булевая функция, $s\in\Nat$. $L(f)\ge s$ тогда и только тогда,
    когда для любого протокола $\Pi$ для $R_f$ существует распределение $\mu$:
    $\IC_\mu(\Pi)\ge \log s$.
\end{corollary}

\begin{theorem}[Храпченко]
    $L(\oplus_n)\ge n^2.$
\end{theorem}
\begin{proof}
    Покажем, что для любого протокола существует распределение $\mu$: $\IC^{ext}_\mu(\Pi)\ge 2\log n$.
    Из этого напрямую следует, что $L(\oplus_n)\ge n^2$.
    Распределение $\mu$ будет равномерным распределением на парах вида $(x,x\oplus e_i)$,
    где $\oplus_n(x) = 0$, а строка $e_i$ имеет единицу в позиции $i$ и нули во всех остальных. 
    Т.е., пары входов из распределения $\mu$ всегда будут отличаться только в одом бите.
    \[
        \IC_\mu^{ext}(\Pi) 
        \ge \IC_\mu^{int}(\Pi)  
         = I(\Pi:X\mid Y) + I(\Pi:Y\mid X).
    \]
    Рассмотрим однои из слагаемых $I(\Pi:X\mid Y)$.
    \[\begin{aligned}
        I(\Pi:X\mid Y) &= H(X\mid Y) - H(X\mid Y,\Pi)\\
                       &= H(i\mid Y) - H(i\mid Y,\Pi)\\
                       &= \log n - 0.
    \end{aligned}\]
    Таким образом $\IC_\mu^{ext}(\Pi) \ge 2\log n$.
\end{proof}
\begin{exercise}
    Докажите, что для любой булевой функции $f$ и любого распределения $\mu$
    существует протокол $\Pi$ для $R_f$: $\IC^{int}_\mu(\Pi) \le 2\log n$.
\end{exercise}
\begin{exercise}
    Будем называть \emph{универсальным отношением} для строк длины $n$ отношение 
    $U_n = \{(x,y,i) \mid x,y\in\bits^n, x_i\neq y_i\}$ (это обобщение понятия
    отношения Карчмера-Вигдерсона). Будем называть \emph{расширенным универсальным
    отношением} для строк длины $n$ отношение $U'_n = U_n\cup \{(x,x,\perp)\mid x\in\bits^n\}$
    (решая коммуникационную задачу для расширенного универсального отношения
    Алиса и Боб могут получить \emph{одинаковые} строки и тогда они должны ответить $\perp$).

    Докажите следующие утверждения:
    \begin{enumerate}
        \item $4\cdot L(U_n) \ge L(U'_n) \ge L(U_n)$.
        \item $L(U'_n) \ge 2^n$.
    \end{enumerate}
\end{exercise}
\begin{exercise}
    Пусть $f:\bits^n\to\bits$ некоторая булева функция. Определим функцию $(\lor_m\circ f): \bits^{m\times n}\to\bits$
    следующим образом: $$(\lor_m\circ f)(\seqn{x}{m}) = f(x_1)\lor f(x_2)\lor \dotsb\lor f(x_m),$$
    где $x_i\in\bits^n$ (т.е. мы определили композицию функция $\lor_m$ и $f$). Докажите, что $L(\lor_m\circ f) =
    m\cdot L(f)$. 
\end{exercise}


\section{Алгоритмический подход}
\subsection{Колмогоровская сложность}
Сколько информации в первых $10^{10}$ знаках числа $\pi$? Её довольно мало,
но сжать такое количество цифр, например, кодированием Хаффмена, не
получится.

\begin{definition}
    Частичная функция $f:\bitstr\to\bitstr$ называется \emph{вычислимой}, если
    существует программа $P$:
    \begin{itemize}
        \item для $\forall x    \in\dom f$: $P(x)$ печатает $f(x)$,
        \item для $\forall x\not\in\dom f$: $P(x)$ не останавливается.
    \end{itemize}
\end{definition}
\begin{definition}
    Пусть $F:\bitstr\to\bitstr$~--- вычислимая функция. \emph{Сложность описания 
    относительно $F$} определяется как \[K_F(x) = \min\{|p| : F(p) = x\}.\]
\end{definition}
\begin{definition}
    Будем говорить, что способ описания $F$ \emph{не хуже} $G$, обозначается
    $F\prec G$, если существует константа $c_G$ такая, что для $\forall x\in\bitstr$ 
    \[K_F(x) \le K_G(x) + c_G.\]
\end{definition}
\begin{theorem}[Соломонова-Колмогорова]\label{thm:solomonov-kolmogorov}
    Существует способ описания (вычислимая функция) $F$ такой, что для любого
    другого способа описания $G$ выполняется $F\prec G$.  
\end{theorem}
Докажем сначала более простое утверждение.
\begin{statement}
    Пусть $F$ и $G$~— два способа описания. Тогда существует способ описания $H$ 
    такой, что $H\prec F$ и $H\prec G$.
\end{statement}
\begin{proof}
    Определим $H$ следующим образом: $H(0x) = F(x)$, $H(1x) = G(x)$ (если
    на каком-то входе $x$ значение $F(x)$ или $G(x)$ не определено, то и $H$ не определено на
    соответствующем входе $0x$ или $1x$). Тогда легко
    проверить, что для любых $x$ верно $K_H(x)\le K_F(x) + 1$ и $K_H(x)\le K_G(x) + 1$.
\end{proof}
\begin{proof}[Доказательство теоремы~\ref{thm:solomonov-kolmogorov}]
    Пронумеруем все программы натуральными числами (программ счётное число). Пусть $F_N$~— 
    это программа с номером $N$ (для машин Тьюринга $N$ называется \emph{номером Гёделя}). 
    Рассмотрим функцию $U(\langle N, x\rangle) = F_N(x)$, где пара $\langle
    N,x\rangle$ закодирована следующим образом $\underbrace{11\dots1}_{N}0x$.
    Тогда 
    \[
        K_U(x)\le K_{F_N}(x) + N + 1.
    \]
    (Для машин Тьюринга $U$~--- это универсальная машина Тьюринга.)
\end{proof}
\begin{definition}
    Будем называть $K(x) = K_U(x)$ \emph{Колмогоровской сложностью $x$}.
\end{definition}

\begin{lemma} Колмогоровская сложность обладает следующими свойствами.
\begin{enumerate}
    \item Существует $c$ такая, что для всех $x$ $K(x)\le |x| + c$. 
    \item Существует $c$ такая, что для всех $x$ $K(xx)\le |x| + c$.
    \item Для любых оптимальных $F_1$ и $F_2$ выполняется $F_1\prec F_2$ и $F_2\prec F_1$,
        т.е. существует такая константа $c$, что 
        \(
            |K_{F_1} - K_{F_2}| \le c.            
        \)
\end{enumerate}
\end{lemma}
\begin{proof} Третье свойство следует из определения. Докажем первые два.
\begin{enumerate}
    \item Рассмотрим $H(x) = x$. Тогда
        \(K(x)\le K_H(x) + c = |x| + c.\)
    \item Рассмотрим $H(p) = p$. Тогда
        \(K(x)\le K_H(xx) + c = |x| + c.\)
\end{enumerate}
\end{proof}

Вопрос: может быть такая длина $n$, что для всех $x\in\{0,1\}^n$ $K(x) < n$.
\begin{statement}
    Для любого $n$ существует $x\in\{0,1\}^n$ такой, что $K(x)\ge n$ (т.е.
    $x$~--- несжимаемый).
\end{statement}
\begin{proof}
    Слов длины $n$ всего $2^n$. Слов сложности меньше $n$ не больше, чем
    программ длины меньше $n$:
    \(
    1+2+\dotsb 2^{n-1} = 2^n - 1 < 2^n.
    \)
\end{proof}
\begin{statement}
    Существует $c$ такой, что для $99\%$ слов длины $n$:
    \[
        n - c \le K(x) \le n + c = |x| + c.
    \]
\end{statement}
\begin{proof}
    Второе неравенство мы уже доказали. Первое неравенство следует из того, что
    слов длины не более $n - c$ всего $1+2+\dotsb 2^{n-c} \le 2^{n - c + 1}$,
    т.е. доля таких слов не может быть больше $2^{-c + 1}$.
    При $c = 11$ доля таких слов меньше $0.01\%$. 
\end{proof}
\begin{statement}
    Не существует вычислимой функции $f:\bitstr\to\bitstr$, которая была бы
    всюду определена и $f(\bar n) = x_n$, где $K(x_n)\ge n$ ($\bar n$ означает
    двоичную запись числа $n$).
\end{statement}
\begin{proof} С одной стороны сложность $x_n$ большая, с другой стороны мы
    можем описать $x_n$ при помощи $\log n$ битов.
    \[
        n\le K(x_n)\le K_f(\bar n) + O(1) \le \log n + O(1).
    \]
\end{proof}
\begin{remark}
    Это утверждение можно усилить, заменив ,,всюду определена`` на ,,определена
    для бесконечного числа входов``. Доказательство останется тем же.
\end{remark}
\begin{corollary}
    Отображение $x\to K(x)$ не является вычислимым.
\end{corollary}
\begin{remark}
    У этого факта есть довольно простое доказательство основанное на парадоксе Берри.
    Этот парадокс состоит в предложении рассмотреть 
    \begin{center}
        наименьшее натуральное число, которое нельзя определить\\ фразой
        из не более чем четырнадцати русских слов.
    \end{center}
    Эта фраза содержит четырнадцать слов и определяет то самое наименьшее число,
    отсюда получаем противоречие. Аналогично, в предположении, что такое отображение
    является вычислимым, первую строку $x$ для которой $K(x)\ge n$ мы можем описать
    при помощи $\log n$ битов.
\end{remark}
\begin{corollary}
    Оптимальный способ описания не является всюду определённой функцией.
\end{corollary}
\begin{corollary}
    Пусть есть некоторая формальная теория, т.ч. в ней можно записать
    `$K(x)>c$'. Для всех достаточно больших $c$ и для всех $x$ формулы
    `$K(x)>c$' недоказуемы (и при этом почти все эти утверждения истины).
\end{corollary}
\begin{proof}
    Если для любого $c$ существует $x$ такое, что `$K(x)>c$' доказуемо,
    тогда перебирая все доказательства мы сможем по $c$ построить $x$.
\end{proof}
\begin{corollary}
    Первая теорема Гёделя о неполноте.
\end{corollary}
\begin{remark}
    Это кроме всего прочего даёт способ с хорошей вероятностью порождать недоказуемые
    утверждения.
\end{remark}

\begin{statement}\label{st:kologorov:entropy}
Пусть $x = \langle{011010010\dotso 10110}\rangle$ длины $n$ содержит
$p\cdot n$ единиц и $(1-p)\cdot n$ нулей, тогда 
\[
    K(x)\le \left(p\cdot\log\frac1p + (1-p)\cdot\log\frac{1}{1-p}\right)\cdot n
        + O(\log n).
\]
\end{statement}
\begin{proof}
    Рассмотрим следующее описание:
    \begin{center}
    $\langle$количество '1', количество '0', номер перестановки с данным числом '1' и '0'$\rangle$.
    \end{center}
Всего перестановок 
    \[
        C_{n}^{pn} = 2^{\left(p\cdot\log\frac1p +
        (1-p)\cdot\log\frac{1}{1-p}\right)\cdot n + O(\log n)}.
    \]
    Т.е. $K(x)\le \left(p\cdot\log\frac1p +
        (1-p)\cdot\log\frac{1}{1-p}\right)\cdot n + O(\log n) = H(p) + O(\log n).$
\end{proof}
\begin{remark}
    В доказательстве важно кодировать эту тройку так, чтобы она однозначно
    разрезалась на три части. Можно, например, удвоить все биты первых
    компонент и добавить разделитель '01'.
\end{remark}
\subsection{Условная Колмогоровская сложность}
\begin{definition} Сложность \emph{условного описания} $x$ при условии $y$ относительно $F$:
    \[K_F(x\mid y) = \min\{|p| : F(p,y) = x\}.\]
\end{definition}
\begin{definition} Условное описание $F$ \emph{не хуже}, чем условное описание $G$,
    $F\prec G$, если существует $c$ такая, что для любый $x$ и $y$
    \[
        K_F(x\mid y)\le K_G(x\mid y) + c.
    \]
\end{definition}
\begin{theorem}
    Существует оптимальный способ описания условного описания $F$ такой, что для любого
    другого способа условного описания $G$ выполняется $F\prec G$.  
\end{theorem}
\begin{definition}
    Сложность оптимального описание $x$ при условии $y$ относительно оптимального способа условного описания
    $K(x\mid y)$ называется \emph{условной Колмогоровской сложностью $x$ при условии $y$}.
\end{definition}
\begin{statement}
    Условная Колмогоровская сложность обладает следующими свойствами.
\begin{enumerate}
    \item $K(x\mid y)\le K(x) + O(1)$.
    \item $K(x\mid y)\le |x| + O(1)$.
    \item Существует такая константа $c$, что для всех $n$, всех $y$ для $99\%$
        слов $x$ длины $n$ выполняется \(|K(x\mid y) - n|\le c.\)
    \item $K(x\mid x) = O(1)$.
    \item Пусть $f$~--- вычислимая функция. Тогда существует $c_f$ такая, что
        для всех $x$ $K(f(x)\mid x)\le c_f$. 
\end{enumerate}
\end{statement}

\subsection{Сложность пары}
Будем обозначать сложность пары $K(x,y) = K(\langle x,y\rangle)$, где
$\langle\cdot,\cdot\rangle$~--- это произвольный вычислимый способ кодирования
пар.
\begin{statement}
    Следующее утверждение \emph{неверно}:
    \[
        \exists c\ \forall x,y\ K(x,y) \le K(x) + K(y\mid x) + c.
    \]
\end{statement}
\begin{proof}
    Докажем от обратного. Пусть $|x| + |y| = n$. Тогда 
    \[
    K(x,y)\le K(x) + K(y \mid x) + c \le |x| + |y| + 2\cdot O(1) + c = n + O(1).
    \]   
    С одной стороны различных пар всего $(n+1)\cdot 2^n$. С
    другой стороны из оценки на сложность следует, что различных описаний пар
    не может быть больше $2^{n + O(1)}$.
\end{proof}  
\begin{theorem}
    \(
        \forall x,y\ K(x,y) \le K(x) + K(y\mid x) + O(\log(|x| + |y|)).
    \)
\end{theorem}
\begin{proof}
    Рассмотрим следующий способ кодирования пар: $\langle
    \overline{\overline{|p|}}01pq\rangle$, где $\overline{\overline{|p|}}$~--- это
    двоичная запись $|p|$, в которой удвоен каждый бит.
\end{proof}
\begin{theorem}[Колмогорова-Левина]\label{thm:kolmogorov-levin}
    $K(x,y) = K(x) + K(y\mid x) + O(\log(|x| + |y|))$.
\end{theorem}
\begin{definition}
    \emph{Взаимная информация $x$ и $y$:}
    \[
        I(x:y) = K(y) - K(y\mid x),
    \]\[
        I(y:x) = K(x) - K(x\mid y).
    \]
\end{definition}
Таким образом теорема Колмогорова-Левина~--- это теорема о симметрии взаимной
информации.
    \[
        I(x:y) = K(x) + K(y) - K(x,y) + O(\log(|x| + |y|)) =  I(y:x).
    \]
\begin{proof}[Доказательство теоремы~\ref{thm:kolmogorov-levin}] Неравенство `$\le$' уже
    доказано. Осталось доказать `$\ge$'.
    \[
    \underbrace{K(x)}_{m} - \underbrace{K(y\mid x)}_l \le \underbrace{K(x,y)}_n
    + \underbrace{O(\log(|x| + |y|))}_{O(\log n)}.
    \]
    Пусть $S = \{( a,b) \mid K(a,b)\le n\}$. Заметим, что $(
    x,y)\in S$ и $|S|\le 2^{n+1}$. 
    Рассмотрим $S_x = \{( x,b) \mid ( x,b) \in S\}$.
    По определению $( x,y)\in S_x$.
    Покажем, что 
    \[
        l = K(y\mid x)\le \log|S_x| + O(\log n).
    \]
    Будем перечислять множество $S$. В процессе этого перечисления мы будем
    получать точки из $S_x$. Для того, чтобы задать $y$ нам нужно указать
    номер $( x,y )$ в этом перечислении. Кроме того, чтобы такое
    перечисление запустить, нам нужно знать число $n$. Получается, что
    \[|S_x| \ge 2^{l - c\cdot\log n} \ge 2^{l'},\]
    где $l'$~--- ближайшее снизу целое, т.е. $l' = \lfloor l - c\cdot\log n\rfloor$.

    Посмотрим ещё раз на перечисление $S$. В процессе перечисления у нас
    возникают ,,тяжёлые сечения``~--- те, в которых число элементов хотя бы
    $2^{l'}$. Для того, чтобы задать сечение $S_x$, нам нужно задать его
    порядковый номер в перечислении $S$ среди всех ,,тяжёлых сечений``. Таким
    образом
    \[
        m = K(x) \le \log(\text{\# тяжёлых сечений}) + O(\log n) + O(\log l').
    \]
    Тяжёлых сечений не больше, чем $|S|/2^{l'}$.
    \[
        m = K(x) \le \log\frac{|S|}{2^{l'}} + O(\log n) = n - l + O(\log n).
    \]
    Таким образом получаем утверждение теоремы: $m + l \le n + O(\log n)$.
\end{proof}
\begin{corollary}
    $|I(x:y) - I(y:x)|\le O(\log K(x,y)).$
\end{corollary}
\begin{remark}
    Выберем $n$ такое, что его двоичная запись несжимаема, 
    т.е. $K(\bar n) = \log n + O(1)$. Возьмём $x\in\{0,1\}^n$ такой, что
    $K(x\mid \bar n) = n + O(1).$ Тогда
    \begin{itemize}
    \item\(
        I(\bar n\mid x) = K(x) - K(x\mid\bar n) = n + O(1) - (n + O(1)) = O(1),
    \)
    \item\(
        I(x\mid\bar n) = K(\bar n) - K(\bar n\mid x) = (\log n + O(1)) - O(1) = \log n + O(1).
    \)
    \end{itemize}
    Т.е. нельзя уменьшить логарифмический зазор в теореме Колмогорова-Левина.
\end{remark}

\begin{exercise}
    $2K(x,y,z)\le K(x,y) + K(x,z) + K(y,z) + O(\log n)$, при $n = |x|+|y|+|z|$.
\end{exercise}
\begin{exercise}
    $K(x,y,z) + K(z)\le K(x,z) + K(y,z) + O(\log n)$, при $n = |x|+|y|+|z|$.
\end{exercise}
\begin{exercise}
    $K(z)\le K(z\mid x) + K(z\mid y) + I(x:y) + O(\log n)$, при $n = |x|+|y|+|z|$.
\end{exercise}

\subsection{Метод несжимаемых объектов}
\begin{definition}
    \emph{Конечный автомат с несколькими головками}~--- это конечный автомат, у
    которого на каждом шаге функция перехода по внутреннему состоянию автомата
    и по символам, на которых находятся головки, возвращает состояние на
    следующем шаге и номера головок, которые нужно сдвинуть, и при этом на каждом 
    шаге сдвигается хотя бы одна головка.
\end{definition}
    Определим класс $\mathcal L_k$~--- класс языков, которые распознаются
    конечными автоматами с $k$ головками.
\begin{theorem}\label{thm:kDFA}
    $\mathcal L_k \subsetneq L_{k+1}.$
\end{theorem}

Определим следующее семейство языков над алфавитом $\{0,1,\#\}$
\[
A_n = \{w_1\#w_2\#\dotsb\#w_n\#w_n\#\dotsb\#w_1\mid w_i\in\bitstr\},
\]
где $w_i\in\bitstr$, $\forall i\in\{1,2,\dotsc,n\}$.

При $n = 1$ для языка $A_1 = \{w_1\#w_1\}$ нужно две головки (по лемме о накачке 
конечный автомат с одной головкой этот язык не распознать).

При $n = 3$ можно распознать с четырьмя головками:
\[
\underset{\fbox{1}}{w_1}\#w_2\#w_3\#\underset{\fbox{2}}{w_3}\#\underset{\fbox{3}}{w_2}\#\underset{\fbox{4}}{w_1}.
\]
Но можно обойтись и тремя головками (придумайте трюк):
\[
\underset{\fbox{1}}{w_1}\#\underset{\fbox{2}}{w_2}\#w_3\#w_3\#\underset{\fbox{3}}{w_2}\#w_1.
\]

Если использовать этот трюк для $k$ головок, то можно было бы распознать
язык $A_n$ для $n \le (k-1) + (k-2) + \dotsb + 1$, т.е. $n =
\frac{k\cdot(k-1)}{2}$. Таким образом конечный автомат с $k$ головками 
распознаёт язык $A_n$ для $n \le \frac{k\cdot(k-1)}{2}$.

\begin{lemma}
    $A_n$ не распознаётся конечным автоматом с $k$ головками,
    если $n > {\frac{k\cdot(k-1)}{2}}$.
\end{lemma}
\begin{proof}
    Будем говорить, что пара головок $(i,j)$ \emph{инспектирует} $w_\ell$, если
    найдётся шаг работы конечного автомата, когда $i$-ая головка читает символ 
    левой копии $w_\ell$, а $j$-ая головка читает символ правой копии $w_\ell$.

    Для любого $x\in A_n$ и для любой пары $(i,j)$ существует не более
    одного блока $w_\ell$ такого, что пара $(i,j)$ инспектирует $w_\ell$.
    Если $n > k\cdot (k-1) / 2$, то найдётся блок, который не инспектируется
    ни одной парой головок. Будем рассматривать некоторый $x\in A_n$ и предположим,
    что блок $w_\ell$ не инспектируется.
    \begin{remark}
        Блок $w_\ell$ не инспектируется, поэтому, пока как какие-то головки находятся в
        левой копии $w_\ell$, то в правой копии никакие головки находится не
        могут.
    \end{remark}

    Запишем \emph{протокол работы автомата на слове $x$ с выделенным $\ell$}.
    Будем записывать состояние автомата каждый раз, когда происходят следующие события:
    \begin{itemize}
        \item вход головки в копию $w_\ell$,
        \item выход головки из копии $w_\ell$.
    \end{itemize}
    Состояние автомата будет описываться внутренним состоянием автомата и
    позициями всех головок.  Будем обозначать такой протокол $\pi(x,\ell)$. 

    Предположим, что для конкретного $x$
    \[
     x = w_1\#w_2\#\dotsb\#w_\ell\#\dotsb\#w_n\#w_n\#\dotsb\#w_\ell\#\dotsb\#w_1,
    \]
    конечный автомат не инспектирует блок $\ell$.
    Рассмотрим вход $x'$ с другим блоком $w'_\ell$:
    \[
     x' = w_1\#w_2\#\dotsb\#w'_\ell\#\dotsb\#w_n\#w_n\#\dotsb\#w'_\ell\#\dotsb\#w_1.
    \]                                           

    \begin{statement}
        Невозможно, что для $x'$ блок $\ell$ тоже не инспектируется, 
        и при этом протоколы равны $\pi(x,\ell) = \pi(x',\ell)$.
    \end{statement}
    \begin{proof}
        Если протоколы равны, то автомат должен и допускать вход 
        \[
        x'' = w_1\#w_2\#\dotsb\#w_\ell\#\dotsb\#w_n\#w_n\#\dotsb\#w'_\ell\#\dotsb\#w_1.
        \]                                         
    Если какие-то головки находятся в $w_\ell$, то автомат на $x''$
    работает как на входе $x$. Если какие-то головки находятся в 
    $w_\ell'$, то автомат работает как на входе $x'$. Следовательно он должен
    принимать $x''\not\in A_n$. Таким образом мы пришли к противоречию. 
    \end{proof}
    Мы показали, что для разных $x$ у нас должны быть разные протоколы. Таким
    образом зная $\ell$ и зная протокол мы можем восстановить $w_\ell$~--- для
    этого нужно знать все остальные блоки и протокол.
    Наше наблюдение можно переписать следующим образом:
    \[
        K(w_\ell\mid w_1,\dots,w_{\ell-1},w_{\ell+1},\dotsc,w_n,\ell,\pi(x,\ell)) 
        = O(1).
    \]
    Будем считать, что все блоки имеют длину $N$. Кроме того мы изначально 
    потребуем, чтобы $x$ был несжимаемым, т.е. $K(x) = K(w_1,w_2,\dotsc,w_n)\ge n\cdot N.$
    Тогда
    \[
        n\cdot N \le K(w_1,\dotsc,w_n) \le 
        \underbrace{(n-1)\cdot N}_{\{w_i\}_{i\neq \ell}} + 
        \underbrace{O(\log n)}_{\ell} + 
        \underbrace{4\cdot k\cdot O(k\log nN)}_{\text{сложность}\ \pi(x,\ell)}.
    \]
    При $N\to\infty$ мы получаем противоречие:  $n\cdot N\le (n-1)N + O(k^2\log nN)$.
\end{proof}
\begin{proof}[Доказательство теоремы \ref{thm:kDFA}]
    Язык $A_{\frac{k\cdot (k + 1)}{2}}$ лежит в $\mathcal L_{k+1}$ 
    и не лежит в $\mathcal L_k.$
\end{proof}

\subsection{Определение случайности}
Если говорить о конечных последовательностях, то совершенно непонятно как 
провести границу между случайными и неслучайными последовательностями. 
Давайте попробуем дать формальное определение
случайной бесконечной последовательности на языке Колмогоровской сложности. Какие свойства
мы хотим от этого определения?

Давайте рассмотрим последовательность $\bar x = x_1x_2x_3\dotso x_n\dotso$.
Естественным было бы получить определение вида $\forall n$ $K(x_1x_2x_3\dotso x_n)\ge n - c$. 
Покажем, что для обычного определения Колмогоровской сложности такое определение не имеет смысла.

\begin{statement}
Для любой последовательности $\bar x = x_1x_2x_3\dotso x_n\dotso$
и существует $n$ такое, что 
\[
    K(x_1,\dotsc,x_n) \le n - \log n + O(1).
\]
(т.е. для любой $c$ существует префикс, такой что $K(x_1,\dotsc,x_n) \le n - c$). 
\end{statement}
\begin{proof}
    Возьмём некоторый префикс длины $k$ и интерпретируем его как двоичную запись
    некоторого числа $m$ (добавим ведущую единицу), и рассмотрим его
    продолжение длины $m$: 
    \[
    \underbrace{1x_1x_2x_3\dotso x_k}_{\overline m}
    \underbrace{x_{k+1}\dotso x_{k + m}}_{y},
    \]
    где $|y| = m$. Пусть $n = m + k$. Тогда утверждается, что
    \[
        K(x_1\dotso x_{m+k}) \le K(y) + O(1) \le m + O(1) \le n - k + O(1) \le n - \log n + O(1).
    \]
    Действительно, зная строку $y$ можно определить $m = |y|$ и приписать
    $\overline m$ в начало без ведущей единицы.
\end{proof}

\begin{definition}
    \emph{Префиксная сложность $x$ относительно $F$}: 
    \[
    KP_F(x) = \min \{|p| : F(p) = x\},
    \]
    где $F$~--- это функция с (бес)префиксной областью определения, т.е. если
    определены $F(p_1)$ и $F(p_2)$, то $p_1\not\sqsubset p_2$.
\end{definition}
\begin{definition} Беспрефиксный способ описания $F$ \emph{не хуже} 
    беспрефиксного способа описания $G$,
    $F\prec G$, если $\exists c\ \forall x\ KP_F(x) \le KP_G(x) + c$.
\end{definition}
\begin{theorem}
    Существует оптимальный способ беспрефиксного описания.
\end{theorem}
\begin{proof} 
    Проблема: не все программы имеют беспрефиксную область определения. Можно
    преобразовать любую программу $\pi_i$ в программу с беспрефиксной областью
    определения $\pi'_i$ таким образом, чтобы
    \begin{itemize}
        \item если $\pi_i$ вычисляла функцию $F_i$ с беспрефиксной областью
            определения, то $\pi_i'$ тоже вычисляет $F_i$,
        \item если $\pi_i$ вычисляла что-то другое, то $\pi_i'$ вычисляет
            некоторую функцию с беспрефиксной областью определения (область
            может быть пустой).
    \end{itemize}
    После этого воспользуемся конструкцией аналогичной
    теореме~\ref{thm:solomonov-kolmogorov} (Соломонова-Кол\-мо\-го\-ро\-ва):
    $UP(\underbrace{11\dotso 1}_n 0p) = \pi_n'(p).$

    Определим работу программы $\pi_n'$: на входе $p$ программа $\pi'_n$ запускает 
    параллельно программу $\pi_n$ на всех входах:
    \[\pi_n(0), \pi_n(1), \pi_n(00),\pi_n(01),\dotsc,\pi_n(p),\dotsc\]
    Если в какой-то момент обнаруживается, что $\pi_n$ имеет не беспрефиксную
    область определения, то $\pi'(p)$ зацикливается. Если же в какой-то
    момент $\pi(p)$ завершается и до этого не было обнаружено нарушение
    беспрефиксности, то $\pi'(p) = \pi(p)$.
\end{proof}
\begin{definition}
    $KP(x) = KP_{UP}(x)$, префиксная сложность относительно $UP$,  
    называется \emph{префиксной Колмогоровской сложностью $x$}.
\end{definition}
\begin{exercise}
    $KP(x,y)\le KP(x) + KP(y) + O(1).$
\end{exercise}


\begin{definition}
    Последовательность $\bar x = x_1x_2\dotso x_n\dotso$ называется \emph{случайной по
    Мартин-Лёфу}, если $\exists c\ \forall n\ KP(x_1\dotso x_n) \ge n-c$.
\end{definition}

\begin{statement}
    Префиксная Колмогоровская сложность обладает следующими свойствами
\begin{itemize}
    \item $KP(x)\le K(x) + 2\log K(x) + O(1)$.
    \item $\sum_{x\in \{0,1\}^k} 2^{-KP(x)} \le 1$.
\end{itemize}
\end{statement}
\begin{proof}\mbox{}
\begin{itemize}
    \item $2\log K(x)$ возникает из-за преобразования строки $p$ 
        в беспрефиксную $p' = \overline{\overline{|p|}}01p$.
    \item Аналогично неравенству Крафта-Макмилана для префиксных кодов.
\end{itemize}
\end{proof}

\begin{theorem}
    Почти все последовательности $\bar x = x_1x_2\dotso x_n\dotso$
    являются случайными по Мартину-Лёфу, т.е. неслучайные имеют меру 0 по мере
    Бернулли.
\end{theorem}
\begin{proof}
    Построим покрывающее множество
    \[
    A_c = \bigcup_{KP(x_1\dotso x_n)\le n-c}\Omega_{x_1\dotso x_n},
    \]
    где $\Omega_p = \{\text{все последовательности продолжающие $p$}\}$. $A_c$
    покрывает все неслучайные по Мартину-Лёфу последовательности. Действительно,
    у любой неслучайной последовательности есть начало, задающее такое
    $\Omega_p$. Какова мера $A_c$?
    \begin{multline*}
    \mu(A_c) \le 
    \sum_{KP(x_1\dotso x_n)\le n-c} 2^{-n} \le 
    \sum_{KP(x_1\dotso x_n)\le n-c} 2^{-KP(x_1\dotso x_n) - c} \le\\\le
    \sum_{x_1\dotso x_n} 2^{-KP(x_1\dotso x_n) - c} =
    2^{-c}\cdot \sum_{x_1\dotso x_n} 2^{-KP(x_1\dotso x_n)} \le 2^{-c}.
    \end{multline*}
    Таким образом по любому $\varepsilon$ мы можем предъявить покрытие
    неслучайных по Мартин-Лёфу последовательностей счётным числом конусов.
\end{proof}
\begin{statement}
    Выполняются следующие свойства случайных по Мартину-Лёфу последовательностей.
\begin{itemize}
    \item Всякая случайная по Мартину-Лёфу последовательность невычислима.
    \item Если $\bar x$ случайная по Мартин-Лёфу, то 
        \[
            \lim_{n\to\infty} \frac{\text{число единиц}}{n} = \frac12.
        \]
\end{itemize}
\end{statement}
\begin{proof}\mbox{}
\begin{itemize}
    \item Если $\bar x$ вычислима, то 
    \[
    KP(x_1\dotso x_n)\le K(x_1\dotso x_n) + 2\log
        K(x_1\dotso x_n) \le \log n + 2 \log \log n + O(1).
    \]
    \item Используется оценка $K(x_1\dots x_n) \le H(p)\cdot n + O(\log n)$ из
    утверждения~\ref{st:kologorov:entropy}.
\end{itemize}
\end{proof}

\begin{exercise}
    Докажите, что следующие последовательности не являются случайными по Мартину-Лёфу:
        \begin{itemize}
            \item $x_10x_30x_50\dotso x_{2n+1}0x_{2n+3}\dotso$,
            \item $x_1x_1x_2x_2x_3x_3\dotso x_{n}x_{n}\dotso$.
        \end{itemize}
\end{exercise}

\begin{theorem}[Закон больших чисел в форма Харди-Литлвуда]
    Для почти всех последовательностей $\bar x = x_1x_2\dotso x_n\dotso$ (с
    вероятностью 1)
    \[
        \left|\frac{x_1+x_2+\dotsb+x_n}{n} - \frac12\right| = 
        O\left(\sqrt{\frac{\ln n}{m}}\right).
    \]
\end{theorem}
\begin{proof}
    Пусть в $x_1\dotso x_n$ всего $p_n\cdot n$ единиц и $(1-p_n)\cdot n$ нулей. 
    \[
        KP(x_1\dotso x_n)\le K(x_1\dotso x_n) + O(\log n) \le 
        H(p)\cdot n + O(\log n).
    \]
    Пусть $p=\frac12 + \delta_n$. Разложим $H(p)$ в ряд в окрестности $\frac12$:
    \[
        H(1/2 + \delta_n)\cdot n = (1 - c_H\cdot\delta_n^2 + o(\delta_n^2))\cdot n
        \le(1 - c'_H\cdot\delta_n^2)\cdot n.
    \]
    Таким образом для случайно последовательности (т.е. с вероятностью 1):
    \[
        n - c \le KP(x_1\dotso x_n)\le n - c'_H\cdot\delta_nn + O(\log n).
    \]
    Получаем, что $\delta_n^2\le O\left(\frac{\log n}{n}\right)$.
\end{proof}
\begin{remark}
    Более сильный закон больших чисел имеет оценку
    $(1+\varepsilon)\sqrt{\frac{2\log\log n}{n}}$.
\end{remark}

\subsection{Применение: алгоритм сложения битовых чисел}
Пусть $\bar x=\overline{x_{n-1}\dotso x_0}$ и $\bar y = \overline{y_{n-1}\dotso y_0}$~---
это два $n$-битных числа.  Предложим алгоритм сложения $\bar x$ и $\bar y$, который делает
$\log n$ операций в среднем (предполагается, что побитовые операции с
$n$-битными числами выполняются за O(1)).

Алгоритм будет устроен следующим образом.
\begin{itemize}
    \item Первая итерация.\\
        Вычисляем $\bar z^{(1)}$: $z_i^{(1)} = x_i \oplus y_i$.\\
        Вычисляем $\bar c^{(1)}$: $c_i^{(1)} = x_{i-1} \land  y_{i-1}$ (вектор переносов).
    \item Итерация $k+1$.\\
        Вычисляем $\bar z^{(k+1)}$: $z_i^{(k+1)} = z_i^{(k)} \oplus c_i^{(k)}$.\\
        Вычисляем $\bar c^{(k+1)}$: $c_i^{(k+1)} = z_{i-1}^{(k)} \land  c_{i-1}^{(k)}$.
\end{itemize}
Итерации заканчиваются, если $\bar c^{(k)} = 0$.

На каком входе мы можем сделать $t$ итераций? Утверждается, что это может произойти
только в том случае, если в $\bar x$ и $\bar y$
есть непрерывные блоки длины $t$, соответствующие биты в которых противоположны,
а после них стоит '$1$'.
\begin{center}
\begin{tikzpicture}[node distance=1cm]
    \node (y) at (0,0) {$\bar y:$};
    \node (x) at (0,1) {$\bar x:$};
    \node[right of=x,draw,minimum width=1cm] (x1) {\strut};
    \node[right of=y,draw,minimum width=1cm] (y1) {\strut};
    \node[right = 0mm of x1] (x2) {$b$};
    \node[right = 0mm of y1] (y2) {$b$};
    \node[right = 0mm of x2,draw,minimum width=1.5cm] (x3) {\strut$v_t\dotso v_1$};
    \node[right = 0mm of y2,draw,minimum width=1.5cm] (y3) {\strut$\bar v_t\dotso\bar v_1$};
    \node[right = 0mm of x3] (x4) {$1$};
    \node[right = 0mm of y3] (y4) {$1$};
    \node[right = 0mm of x4,draw,minimum width=1cm]  {\strut};
    \node[right = 0mm of y4,draw,minimum width=1cm]  {\strut};
    \node[below = 1mm of y4] {$j$};
\end{tikzpicture}
\end{center}

Так как у $\bar x$ и $\bar y$ есть общий блок битов длины $t$, то
\[
    K(\bar x\mid\bar y)\le (n-t-1) + \underbrace{\log n}_{j} + O(1).
\]
Отсюда $t\le n - K(\bar x\mid\bar y) + \log n + O(1).$ Среднее количество
итераций в алгоритме можно оценить как
\[
    \sum_t t\cdot [\text{доля пар $(x,y)$ с общим блоком длины $t$}]
\]

Введём обозначение
$K(\bar x\mid \bar y)\le n - \underbrace{t + 1 + \log n + O(1)}_{s}$ и будем
называть $s$ \emph{дефектом случайности}, т.е. $s = t - 1 - \log n + O(1)$ и
$t\le s + \log n + O(1)$.

Доля пар $(\bar x, \bar y)$ таких, что $K(\bar x\mid \bar y) = n - s$
примерно равна $p_s\approx 2^{-s}$. Таким образом среднее количество итераций в
алгоритме не больше, чем
\[
    \sum_s (s + \log n)\cdot 2^{-s} = \sum_s 2^{-s}\cdot\log n + \sum_s \frac{s}{2^s}
    = \log n + O(1).
\]

\end{document}
% vim: set tw=120:
